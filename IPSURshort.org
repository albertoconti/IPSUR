#+TITLE:     Introduction to Probability and Statistics Using R
#+AUTHOR:    G. Jay Kerns
#+EMAIL:     gkerns@ysu.edu
#+LANGUAGE:  en
#+OPTIONS:   H:5 toc:3 \n:nil @:t ::t |:t ^:{} -:t f:nil *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:nil
#+EXPORT_SELECT_TAGS: introps
#+PROPERTY: session *R*
#+PROPERTY: exports results
#+PROPERTY: results value raw
#+PROPERTY: cache no
#+PROPERTY: tangle yes
#+LaTeX_CLASS: scrbook
#+LaTeX_CLASS_OPTIONS: [captions=tableheading]
#+LaTeX_CLASS_OPTIONS: [10pt,english]
#+LaTeX_HEADER: \input{tex/preamble}
#+LATEX: \input{tex/frontmatter}
#+LATEX: \input{tex/preface-second}
#+LATEX: \input{tex/preface-first}
#+INCLUDE: "R/prelim.R" src R :exports none :results hide

* An Introduction to Probability and Statistics                     :introps:
:PROPERTIES:
:tangle: R/introps.R
:END:
#+LaTeX: \pagenumbering{arabic}

#+LaTeX: \noindent 
This chapter has proved to be the hardest to write, by far. The
trouble is that there is so much to say -- and so many people have
already said it so much better than I could. When I get something I
like I will release it here.

In the meantime, there is a lot of information already available to a
person with an Internet connection. I recommend to start at Wikipedia,
which is not a flawless resource but it has the main ideas with links
to reputable sources.

In my lectures I usually tell stories about Fisher, Galton, Gauss,
Laplace, Quetelet, and the Chevalier de Mere.

** Probability

The common folklore is that probability has been around for millennia
but did not gain the attention of mathematicians until approximately
1654 when the Chevalier de Mere had a question regarding the fair
division of a game's payoff to the two players, supposing the game had
to end prematurely.

** Statistics

Statistics concerns data; their collection, analysis, and
interpretation. In this book we distinguish between two types of
statistics: descriptive and inferential.

Descriptive statistics concerns the summarization of data. We have a
data set and we would like to describe the data set in multiple
ways. Usually this entails calculating numbers from the data, called
descriptive measures, such as percentages, sums, averages, and so
forth.

Inferential statistics does more. There is an inference associated
with the data set, a conclusion drawn about the population from which
the data originated.

I would like to mention that there are two schools of thought of
statistics: frequentist and bayesian. The difference between the
schools is related to how the two groups interpret the underlying
probability (see Section [[sec-Interpreting-Probabilities][Interpreting Probabilities]]). The frequentist
school gained a lot of ground among statisticians due in large part to
the work of Fisher, Neyman, and Pearson in the early twentieth
century. That dominance lasted until inexpensive computing power
became widely available; nowadays the bayesian school is garnering
more attention and at an increasing rate.

This book is devoted mostly to the frequentist viewpoint because that
is how I was trained, with the conspicuous exception of Sections
[[sec-Bayes-Rule][Bayes' Rule]] and [[sec-Conditional-Distributions][Conditional Distributions]]. I plan to add more bayesian
material in later editions of this book.

#+LaTeX: \newpage{}

** Exercises
#+LaTeX: \setcounter{thm}{0}

* An Introduction to R                                               :introR:
:PROPERTIES:
:tangle: R/introR.R
:CUSTOM_ID: cha-introduction-to-R
:END:

#+BEGIN_SRC R :exports none :eval never
# Chapter: Introduction to R
# All code released under GPL Version 3
#+END_SRC

Every \(\mathsf{R}\) book I have ever seen has had a section/chapter
that is an introduction to \(\mathsf{R}\), and so does this one.  The
goal of this chapter is for a person to get up and running, ready for
the material that follows.  See Section [[sec-External-Resources][External Resources]] for links
to other material which the reader may find useful.

*What do I want them to know?*
- Where to find \(\mathsf{R}\) to install on a home computer, and a
  few comments to help with the usual hiccups that occur when
  installing something.
- Abbreviated remarks about the available options to interact with
  \(\mathsf{R}\).
- Basic operations (arithmetic, entering data, vectors) at the command
  prompt.
- How and where to find help when they get in trouble.
- Other little shortcuts I am usually asked when introducing
  \(\mathsf{R}\).

** Downloading and Installing \(\mathsf{R}\)
:PROPERTIES:
:CUSTOM_ID: sec-download-install-R
:END:  

The instructions for obtaining \(\mathsf{R}\) largely depend on the
user's hardware and operating system. The \(\mathsf{R}\) Project has
written an \(\mathsf{R}\) Installation and Administration manual with
complete, precise instructions about what to do, together with all
sorts of additional information. The following is just a primer to get
a person started.

*** Installing \(\mathsf{R}\) 

Visit one of the links below to download the latest version of \(\mathsf{R}\) 
for your operating system:

- Microsoft Windows: :: http://cran.r-project.org/bin/windows/base/
- MacOS: :: http://cran.r-project.org/bin/macosx/
- Linux: :: http://cran.r-project.org/bin/linux/

On Microsoft Windows, click the =R-x.y.z.exe= installer to start
installation. When it asks for "Customized startup options", specify
=Yes=. In the next window, be sure to select the SDI (single document
interface) option; this is useful later when we discuss three
dimensional plots with the =rgl= package \cite{rgl}.

**** Installing \(\mathsf{R}\) on a USB drive (Windows)

With this option you can use \(\mathsf{R}\) portably and without
administrative privileges. There is an entry in the \(\mathsf{R}\) for
Windows FAQ about this. Here is the procedure I use:
1. Download the Windows installer above and start installation as
   usual. When it asks /where/ to install, navigate to the top-level
   directory of the USB drive instead of the default =C= drive.
2. When it asks whether to modify the Windows registry, uncheck the
   box; we do NOT want to tamper with the registry.
3. After installation, change the name of the folder from =R-x.y.z= to
   just plain \(\mathsf{R}\). (Even quicker: do this in step 1.)
4. [[http://ipsur.r-forge.r-project.org/book/download/R.exe][Download this shortcut]] and move it to the top-level directory of
   the USB drive, right beside the \(\mathsf{R}\) folder, not inside
   the folder. Use the downloaded shortcut to run \(\mathsf{R}\).

Steps 3 and 4 are not required but save you the trouble of navigating
to the =R-x.y.z/bin= directory to double-click =Rgui.exe= every time
you want to run the program. It is useless to create your own shortcut
to =Rgui.exe=. Windows does not allow shortcuts to have relative
paths; they always have a drive letter associated with them. So if you
make your own shortcut and plug your USB drive into some /other/
machine that happens to assign your drive a different letter, then
your shortcut will no longer be pointing to the right place.

*** Installing and Loading Add-on Packages
:PROPERTIES:
:CUSTOM_ID: sub-installing-loading-packages
:END:

There are /base/ packages (which come with \(\mathsf{R}\)
automatically), and /contributed/ packages (which must be downloaded
for installation). For example, on the version of \(\mathsf{R}\) being
used for this document the default base packages loaded at startup are

#+BEGIN_SRC R :exports both :results output pp
getOption("defaultPackages")
#+END_SRC

The base packages are maintained by a select group of volunteers,
called \(\mathsf{R}\) Core. In addition to the base packages, there
are literally thousands of additional contributed packages written by
individuals all over the world. These are stored worldwide on mirrors
of the Comprehensive \(\mathsf{R}\) Archive Network, or =CRAN= for
short. Given an active Internet connection, anybody is free to
download and install these packages and even inspect the source code.

To install a package named =foo=, open up \(\mathsf{R}\) and type
=install.packages("foo")=\index{install.packages@\texttt{install.packages}}. To
install =foo= and additionally install all of the other packages on
which =foo= depends, instead type =install.packages("foo", depends =
TRUE)=.

The general command =install.packages()= will (on most operating
systems) open a window containing a huge list of available packages;
simply choose one or more to install.

No matter how many packages are installed onto the system, each one
must first be loaded for use with the
=library=\index{library@\texttt{library}} function. For instance, the
=foreign= package \cite{foreign} contains all sorts of functions
needed to import data sets into \(\mathsf{R}\) from other software
such as SPSS, SAS, /etc/. But none of those functions will be
available until the command =library("foreign")= is issued.

Type =library()= at the command prompt (described below) to see a list
of all available packages in your library.

For complete, precise information regarding installation of
\(\mathsf{R}\) and add-on packages, see the [[http://cran.r-project.org/manuals.html][\(\mathsf{R}\)
Installation and Administration manual]].

** Communicating with \(\mathsf{R}\)
:PROPERTIES:
:CUSTOM_ID: sec-Communicating-with-R
:END:

*** One line at a time

This is the most basic method and is the first one that beginners will use.
- RGui (Microsoft \(\circledR\) Windows)
- Terminal
- Emacs/ESS, XEmacs
- JGR

*** Multiple lines at a time

For longer programs (called /scripts/) there is too much code to write
all at once at the command prompt. Furthermore, for longer scripts it
is convenient to be able to only modify a certain piece of the script
and run it again in \(\mathsf{R}\). Programs called /script editors/
are specially designed to aid the communication and code writing
process. They have all sorts of helpful features including
\(\mathsf{R}\) syntax highlighting, automatic code completion,
delimiter matching, and dynamic help on the \(\mathsf{R}\) functions
as they are being written. Even more, they often have all of the text
editing features of programs like Microsoft\(\circledR\)Word. Lastly,
most script editors are fully customizable in the sense that the user
can customize the appearance of the interface to choose what colors to
display, when to display them, and how to display them.

- \(\mathsf{R}\) Editor (Windows):\index{R Editor@\textsf{R} Editor} :: In
     Microsoft\(\circledR\) Windows, \(\mathsf{R}\) Gui has its own
     built-in script editor, called \(\mathsf{R}\) Editor. From the
     console window, select =File= \(\triangleright\) =New Script=. A
     script window opens, and the lines of code can be written in the
     window. When satisfied with the code, the user highlights all of
     the commands and presses \textsf{Ctrl+R}. The commands are
     automatically run at once in \(\mathsf{R}\) and the output is
     shown. To save the script for later, click =File=
     \(\triangleright\) =Save as...= in \(\mathsf{R}\) Editor. The
     script can be reopened later with =File= \(\triangleright\)}
     =Open Script...= in =RGui=. Note that \(\mathsf{R}\) Editor does
     not have the fancy syntax highlighting that the others do.
- \(\mathsf{R}\) WinEdt:\index{RWinEdt@\textsf{R}WinEdt} :: This
     option is coordinated with WinEdt for \LaTeX{} and has additional
     features such as code highlighting, remote sourcing, and a ton of
     other things. However, one first needs to download and install a
     shareware version of another program, WinEdt, which is only free
     for a while -- pop-up windows will eventually appear that ask for
     a registration code. \(\mathsf{R}\) WinEdt is nevertheless a very
     fine choice if you already own WinEdt or are planning to purchase
     it in the near future.
- Tinn \(\mathsf{R}\) / Sciviews K:\index{Tinn R@Tinn \textsf{R}}\index{Sciviews K} :: This
     one is completely free and has all of the above mentioned options
     and more. It is simple enough to use that the user can virtually
     begin working with it immediately after installation. But Tinn
     \(\mathsf{R}\) proper is only available for
     Microsoft\(\circledR\) Windows operating systems. If you are on
     MacOS or Linux, a comparable alternative is Sci-Views - Komodo
     Edit.
- Emacs/ESS:\index{Emacs}\index{ESS} :: Emacs is an all purpose text
     editor. It can do absolutely anything with respect to modifying,
     searching, editing, and manipulating, text. And if Emacs can't do
     it, then you can write a program that extends Emacs to do
     it. Once such extension is called =ESS=, which stands for
     /E/-macs /S/-peaks /S/-tatistics. With ESS a person can speak to
     \(\mathsf{R}\), do all of the tricks that the other script
     editors offer, and much, much, more. Please see the following for
     installation details, documentation, reference cards, and a whole
     lot more: http://ess.r-project.org.  /Fair warning/: if you want
     to try Emacs and if you grew up with Microsoft\(\circledR\)
     Windows or Macintosh, then you are going to need to relearn
     everything you thought you knew about computers your whole
     life. (Or, since Emacs is completely customizable, you can
     reconfigure Emacs to behave the way you want.) I have personally
     experienced this transformation and I will never go back.
- JGR (read ``Jaguar''):\index{JGR} :: This one has the bells and
     whistles of =RGui= plus it is based on Java, so it works on
     multiple operating systems. It has its own script editor like
     \(\mathsf{R}\) Editor but with additional features such as syntax
     highlighting and code-completion. If you do not use
     Microsoft\(\circledR\) Windows (or even if you do) you definitely
     want to check out this one.
- Kate, Bluefish, /etc/ :: There are literally dozens of other text
     editors available, many of them free, and each has its own
     (dis)advantages. I only have mentioned the ones with which I have
     had substantial personal experience and have enjoyed at some
     point. Play around, and let me know what you find.

*** Graphical User Interfaces (GUIs)

By the word ``GUI'' I mean an interface in which the user communicates
with \(\mathsf{R}\) by way of points-and-clicks in a menu of some
sort. Again, there are many, many options and I only mention ones that
I have used and enjoyed. Some of the other more popular script editors
can be downloaded from the \(\mathsf{R}\)-Project website at
http://www.sciviews.org/_rgui/. On the left side of the screen
(under *Projects*) there are several choices available.

- \(\mathsf{R}\) Commander :: provides\index{R Commander@\textsf{R}
     Commander} a point-and-click interface to many basic statistical
     tasks. It is called the ``Commander'' because every time one
     makes a selection from the menus, the code corresponding to the
     task is listed in the output window. One can take this code,
     copy-and-paste it to a text file, then re-run it again at a later
     time without the \(\mathsf{R}\) Commander's assistance. It is
     well suited for the introductory level. =Rcmdr= \cite{Rcmdr} also
     allows for user-contributed ``Plugins'' which are separate
     packages on =CRAN= that add extra functionality to the =Rcmdr=
     package. The plugins are typically named with the prefix
     =RcmdrPlugin= to make them easy to identify in the =CRAN= package
     list. One such plugin is the =RcmdrPlugin.IPSUR= package
     \cite{RcmdrPlugin.IPSUR} which accompanies this text.
- Poor Man's GUI\index{Poor Man's GUI} :: is an alternative to the
     =Rcmdr= which is based on GTk instead of Tcl/Tk. It has been a
     while since I used it but I remember liking it very much when I
     did. One thing that stood out was that the user could
     drag-and-drop data sets for plots. See here for more information:
     http://wiener.math.csi.cuny.edu/pmg/.
- Rattle\index{Rattle} :: is a data mining toolkit which was designed
     to manage/analyze very large data sets, but it provides enough
     other general functionality to merit mention here. See
     \cite{rattle} for more information.
- Deducer\index{Deducer} :: is relatively new and shows promise from
     what I have seen, but I have not actually used it in the
     classroom yet.

** Basic \(\mathsf{R}\) Operations and Concepts
:PROPERTIES:
:CUSTOM_ID: sec-Basic-R-Operations
:END:

The \(\mathsf{R}\) developers have written an introductory document
entitled ``An Introduction to \(\mathsf{R}\)''. There is a sample
session included which shows what basic interaction with
\(\mathsf{R}\) looks like. I recommend that all new users of
\(\mathsf{R}\) read that document, but bear in mind that there are
concepts mentioned which will be unfamiliar to the beginner.

Below are some of the most basic operations that can be done with
\(\mathsf{R}\). Almost every book about \(\mathsf{R}\) begins with a
section like the one below; look around to see all sorts of things
that can be done at this most basic level.

*** Arithmetic
:PROPERTIES:
:CUSTOM_ID: sub-Arithmetic
:END:

#+BEGIN_SRC R :exports both :results output pp  
2 + 3       # add
4 * 5 / 6   # multiply and divide
7^8         # 7 to the 8th power
#+END_SRC

Notice the comment character =#=\index{#@\texttt{\#}}. Anything typed
after a =#= symbol is ignored by \(\mathsf{R}\). We know that \(20/6\)
is a repeating decimal, but the above example shows only 7 digits. We
can change the number of digits displayed with
=options=\index{options@\texttt{options}}:

#+BEGIN_SRC R :exports both :results output pp 
options(digits = 16)
10/3                 # see more digits
sqrt(2)              # square root
exp(1)               # Euler's constant, e
pi       
options(digits = 7)  # back to default
#+END_SRC

Note that it is possible to set =digits=\index{digits@\texttt{digits}}
up to 22, but setting them over 16 is not recommended (the extra
significant digits are not necessarily reliable). Above notice the
=sqrt=\index{sqrt@\texttt{sqrt}} function for square roots and the
=exp=\index{exp@\texttt{exp}} function for powers of \(\mathrm{e}\),
Euler's number.

*** Assignment, Object names, and Data types
:PROPERTIES:
:CUSTOM_ID: sub-Assignment-Object-names
:END:

It is often convenient to assign numbers and values to variables
(objects) to be used later. The proper way to assign values to a
variable is with the =<-= operator (with a space on either side). The
~=~ symbol works too, but it is recommended by the \(\mathsf{R}\)
masters to reserve ~=~ for specifying arguments to functions
(discussed later). In this book we will follow their advice and use
=<-= for assignment. Once a variable is assigned, its value can be
printed by simply entering the variable name by itself.

#+BEGIN_SRC R :exports both :results output pp 
x <- 7*41/pi   # don't see the calculated value
x              # take a look
#+END_SRC

When choosing a variable name you can use letters, numbers, dots
``\texttt{.}'', or underscore ``\texttt{\_}'' characters. You cannot
use mathematical operators, and a leading dot may not be followed by a
number. Examples of valid names are: =x=, =x1=, =y.value=, and
=!y_hat=. (More precisely, the set of allowable characters in object
names depends on one's particular system and locale; see An
Introduction to \(\mathsf{R}\) for more discussion on this.)

Objects can be of many /types/, /modes/, and /classes/. At this level,
it is not necessary to investigate all of the intricacies of the
respective types, but there are some with which you need to become
familiar:
- integer: :: the values \(0\), \(\pm1\), \(\pm2\), ...; these are
              represented exactly by \(\mathsf{R}\).
- double: :: real numbers (rational and irrational); these numbers are
             not represented exactly (save integers or fractions with
             a denominator that is a power of 2, see
             \cite{Venables2010}).
- character: :: elements that are wrapped with pairs of ="= or ';
- logical: :: includes =TRUE=, =FALSE=, and =NA= (which are reserved
              words); the =NA=\index{NA@\texttt{NA}} stands for ``not
              available'', /i.e./, a missing value.

You can determine an object's type with the =typeof=
\index{typeof@\texttt{typeof}} function. In addition to the above,
there is the =complex= \index{complex@\texttt{complex}}
\index{as.complex@\texttt{as.complex}} data type:

#+BEGIN_SRC R :exports both :results output pp 
sqrt(-1)              # isn't defined
sqrt(-1+0i)           # is defined
sqrt(as.complex(-1))  # same thing
(0 + 1i)^2            # should be -1
typeof((0 + 1i)^2)
#+END_SRC

Note that you can just type =(1i)^2= to get the same answer. The
=NaN=\index{NaN@\texttt{NaN}} stands for ``not a number''; it is
represented internally as =double=\index{double}.

*** Vectors
:PROPERTIES:
:CUSTOM_ID: sub-Vectors
:END:

All of this time we have been manipulating vectors of length 1. Now
let us move to vectors with multiple entries.

**** Entering data vectors

*The long way:*\index{c@\texttt{c}} If you would like to enter the
data =74,31,95,61,76,34,23,54,96= into \(\mathsf{R}\), you may create
a data vector with the =c= function (which is short for
/concatenate/).

#+BEGIN_SRC R :exports both :results output pp 
x <- c(74, 31, 95, 61, 76, 34, 23, 54, 96)
x
#+END_SRC

The elements of a vector are usually coerced by \(\mathsf{R}\) to the
the most general type of any of the elements, so if you do =c(1, "2")=
then the result will be =c("1", "2")=.

*A shorter way:* \index{scan@\texttt{scan}}: The =scan= method is
useful when the data are stored somewhere else. For instance, you may
type =x <- scan()= at the command prompt and \(\mathsf{R}\) will
display =1:= to indicate that it is waiting for the first data
value. Type a value and press =Enter=, at which point \(\mathsf{R}\)
will display =2:=, and so forth. Note that entering an empty line
stops the scan. This method is especially handy when you have a column
of values, say, stored in a text file or spreadsheet. You may copy and
paste them all at the =1:= prompt, and \(\mathsf{R}\) will store all
of the values instantly in the vector =x=.

*Repeated data; regular patterns:* the =seq=\index{seq@\texttt{seq}}
function will generate all sorts of sequences of numbers. It has the
arguments =from=, =to=, =by=, and =length.out= which can be set in
concert with one another. We will do a couple of examples to show you
how it works.

#+BEGIN_SRC R :exports both :results output pp 
seq(from = 1, to = 5)
seq(from = 2, by = -0.1, length.out = 4)
#+END_SRC

Note that we can get the first line much quicker with the colon
operator.

#+BEGIN_SRC R :exports both :results output pp 
1:5
#+END_SRC

The vector =LETTERS=\index{LETTERS@\texttt{LETTERS}} has the 26
letters of the English alphabet in uppercase and
=letters=\index{letters@\texttt{letters}} has all of them in
lowercase.

**** Indexing data vectors

Sometimes we do not want the whole vector, but just a piece of it. We
can access the intermediate parts with the =[]=
\index{[]@\texttt{{[}{]}}} operator. Observe (with =x= defined above)

#+BEGIN_SRC R :exports both :results output pp 
x[1]
x[2:4]
x[c(1,3,4,8)]
x[-c(1,3,4,8)]
#+END_SRC

Notice that we used the minus sign to specify those elements that we
do /not/ want.

#+BEGIN_SRC R :exports both :results output pp 
LETTERS[1:5]
letters[-(6:24)]
#+END_SRC

*** Functions and Expressions
:PROPERTIES:
:CUSTOM_ID: sub-Functions-and-Expressions
:END:

A function takes arguments as input and returns an object as
output. There are functions to do all sorts of things. We show some
examples below.

#+BEGIN_SRC R :exports both :results output pp 
x <- 1:5
sum(x)
length(x)
min(x)
mean(x)      # sample mean
sd(x)        # sample standard deviation
#+END_SRC

It will not be long before the user starts to wonder how a particular
function is doing its job, and since \(\mathsf{R}\) is open-source,
anybody is free to look under the hood of a function to see how things
are calculated. For detailed instructions see the article ``Accessing
the Sources'' by Uwe Ligges \cite{Ligges2006}. In short:

*Type the name of the function* without any parentheses or
arguments. If you are lucky then the code for the entire function will
be printed, right there looking at you. For instance, suppose that we
would like to see how the =intersect=
\index{intersect@\texttt{intersect}} function works:

#+BEGIN_SRC R :exports both :results output pp 
intersect
#+END_SRC

*If instead* it shows =UseMethod(something)=
\index{UseMethod@\texttt{UseMethod}} then you will need to choose the
/class/ of the object to be inputted and next look at the /method/
that will be /dispatched/ to the object. For instance, typing
=rev=\index{rev@\texttt{rev}} says

#+BEGIN_SRC R :exports both :results output pp 
rev
#+END_SRC

The output is telling us that there are multiple methods associated
with the =rev= function. To see what these are, type

#+BEGIN_SRC R :exports both :results output pp 
methods(rev)
#+END_SRC

Now we learn that there are two different =rev(x)= functions, only one
of which being chosen at each call depending on what =x= is. There is
one for =dendrogram= objects and a =default= method for everything
else. Simply type the name to see what each method does. For example,
the =default= method can be viewed with

#+BEGIN_SRC R :exports both :results output pp 
rev.default
#+END_SRC

*Some functions are hidden* by a /namespace/ (see An Introduction to
\(\mathsf{R}\) \cite{Venables2010}), and are not visible on the first
try. For example, if we try to look at the code for =wilcox.test=
\index{wilcox.test@\texttt{wilcox.test}} (see Chapter [[cha-Nonparametric-Statistics][Nonparametric
Statistics]]) we get the following:

#+BEGIN_SRC R :exports both :results output pp 
wilcox.test
methods(wilcox.test)
#+END_SRC

If we were to try =wilcox.test.default= we would get a ``not found''
error, because it is hidden behind the namespace for the package
=stats= \cite{stats} (shown in the last line when we tried
=wilcox.test=). In cases like these we prefix the package name to the
front of the function name with three colons; the command
=stats:::wilcox.test.default= will show the source code, omitted here
for brevity.

*If it shows* =.Internal(something)=
\index{.Internal@\texttt{.Internal}} or =.Primitive(something)=
\index{.Primitive@\texttt{.Primitive}}, then it will be necessary to
download the source code of \(\mathsf{R}\) (which is /not/ a binary
version with an =.exe= extension) and search inside the code
there. See Ligges \cite{Ligges2006} for more discussion on this. An
example is =exp=:

#+BEGIN_SRC R :exports both :results output pp 
exp
#+END_SRC

Be warned that most of the =.Internal= functions are written in other
computer languages which the beginner may not understand, at least
initially.

** Getting Help
:PROPERTIES:
:CUSTOM_ID: sec-Getting-Help
:END:

When you are using \(\mathsf{R}\), it will not take long before you
find yourself needing help. Fortunately, \(\mathsf{R}\) has extensive
help resources and you should immediately become familiar with
them. Begin by clicking =Help= on =RGui=. The following options are
available.
- Console: :: gives useful shortcuts, for instance, =Ctrl+L=, to clear
              the \(\mathsf{R}\) console screen.
- FAQ on \(\mathsf{R}\): :: frequently asked questions concerning
     general \(\mathsf{R}\) operation.
- FAQ on \(\mathsf{R}\) for Windows: :: frequently asked questions
     about \(\mathsf{R}\), tailored to the Microsoft Windows operating
     system.
- Manuals: :: technical manuals about all features of the
              \(\mathsf{R}\) system including installation, the
              complete language definition, and add-on packages.
- \(\mathsf{R}\) functions (text)...: :: use this if you know the
     /exact/ name of the function you want to know more about, for
     example, =mean= or =plot=. Typing =mean= in the window is
     equivalent to typing =help("mean")=\index{help@\texttt{help}} at
     the command line, or more simply,
     =?mean=\index{?@\texttt{?}}. Note that this method only works if
     the function of interest is contained in a package that is
     already loaded into the search path with =library=.
- HTML Help: :: use this to browse the manuals with point-and-click
                links. It also has a Search Engine \& Keywords for
                searching the help page titles, with point-and-click
                links for the search results. This is possibly the
                best help method for beginners. It can be started from
                the command line with the command
                =help.start()=\index{help.start@\texttt{help.start}}.
- Search help ...: :: use this if you do not know the exact name of
     the function of interest, or if the function is in a package that
     has not been loaded yet. For example, you may enter =plo= and a
     text window will return listing all the help files with an alias,
     concept, or title matching `=plo=' using regular expression
     matching; it is equivalent to typing
     =help.search("plo")=\index{help.search@\texttt{help.search}} at
     the command line. The advantage is that you do not need to know
     the exact name of the function; the disadvantage is that you
     cannot point-and-click the results. Therefore, one may wish to
     use the HTML Help search engine instead. An equivalent way is
     =??plo=\index{??@\texttt{??}} at the command line.
- search.r-project.org ...: :: this will search for words in help
     lists and email archives of the \(\mathsf{R}\) Project. It can be
     very useful for finding other questions that other users have
     asked.
- Apropos ...: :: use this for more sophisticated partial name
                  matching of functions. See
                  =?apropos=\index{apropos@\texttt{apropos}} for
                  details.

On the help pages for a function there are sometimes ``Examples''
listed at the bottom of the page, which will work if copy-pasted at
the command line (unless marked otherwise). The =example=
\index{example@\texttt{example}} function will run the code
automatically, skipping the intermediate step. For instance, we may
try =example(mean)= to see a few examples of how the =mean= function
works.

*** \(\mathsf{R}\) Help Mailing Lists

There are several mailing lists associated with \(\mathsf{R}\), and
there is a huge community of people that read and answer questions
related to \(\mathsf{R}\). See [[http://www.r-project.org/mail.html][here]] for an idea of what is
available. Particularly pay attention to the bottom of the page which
lists several special interest groups (SIGs) related to
\(\mathsf{R}\).

Bear in mind that \(\mathsf{R}\) is free software, which means that it
was written by volunteers, and the people that frequent the mailing
lists are also volunteers who are not paid by customer support
fees. Consequently, if you want to use the mailing lists for free
advice then you must adhere to some basic etiquette, or else you may
not get a reply, or even worse, you may receive a reply which is a bit
less cordial than you are used to. Below are a few considerations:
1. Read the [[http://cran.r-project.org/faqs.html][FAQ]]. Note that there are different FAQs for different
   operating systems. You should read these now, even without a
   question at the moment, to learn a lot about the idiosyncrasies of
   \(\mathsf{R}\).
2. Search the archives. Even if your question is not a FAQ, there is a
   very high likelihood that your question has been asked before on
   the mailing list. If you want to know about topic =foo=, then you
   can do =RSiteSearch("foo")=\index{RSiteSearch@\texttt{RSiteSearch}}
   to search the mailing list archives (and the online help) for it.
3. Do a Google search and an \texttt{RSeek.org} search.

If your question is not a FAQ, has not been asked on
\(\mathsf{R}\)-help before, and does not yield to a Google (or
alternative) search, then, and only then, should you even consider
writing to \(\mathsf{R}\)-help. Below are a few additional
considerations.

- Read the [[http://www.r-project.org/posting-guide.html][posting guide]] before posting. This will save you a lot of
  trouble and pain.
- Get rid of the command prompts (=>=) from output. Readers of your
  message will take the text from your mail and copy-paste into an
  \(\mathsf{R}\) session. If you make the readers' job easier then it
  will increase the likelihood of a response.
- Questions are often related to a specific data set, and the best way
  to communicate the data is with a =dump=\index{dump@\texttt{dump}}
  command. For instance, if your question involves data stored in a
  vector =x=, you can type =dump("x","")= at the command prompt and
  copy-paste the output into the body of your email message. Then the
  reader may easily copy-paste the message from your email into
  \(\mathsf{R}\) and =x= will be available to him/her.
- Sometimes the answer the question is related to the operating system
  used, the attached packages, or the exact version of \(\mathsf{R}\)
  being used. The =sessionInfo()=
  \index{sessionInfo@\texttt{sessionInfo}} command collects all of
  this information to be copy-pasted into an email (and the Posting
  Guide requests this information). See Appendix [[cha-R-Session-Information][R Session Info]] for an
  example.

** External Resources
:PROPERTIES:
:CUSTOM_ID: sec-External-Resources
:END:

There is a mountain of information on the Internet about
\(\mathsf{R}\). Below are a few of the important ones.
- The \(\mathsf{R}\)- Project for Statistical Computing\index{The
  R-Project@The \textsf{R}-Project}: Go [[http://www.r-project.org/][there]] first.
- The Comprehensive \(\mathsf{R}\) Archive Network\index{CRAN}: [[http://cran.r-project.org/][That
  is where]] \(\mathsf{R}\) is stored along with thousands of
  contributed packages. There are also loads of contributed
  information (books, tutorials, /etc/.). There are mirrors all over
  the world with duplicate information.
- \(\mathsf{R}\)-Forge\index{R-Forge@\textsf{R}-Forge}: [[http://r-forge.r-project.org/][This is
  another location]] where \(\mathsf{R}\) packages are stored. Here you
  can find development code which has not yet been released to =CRAN=.
- \(\mathsf{R}\)-Wiki\index{R-Wiki@\textsf{R}-Wiki}: There are many
  tips, tricks, and general advice [[http://wiki.r-project.org/rwiki/doku.php][listed here]]. If you find a trick of
  your own, login and share it with the world.
- Other: the [[http://addictedtor.free.fr/graphiques/][\(\mathsf{R}\) Graph Gallery]]\index{R Graph
  Gallery@\textsf{R} Graph Gallery} and [[http://bm2.genes.nig.ac.jp/RGM2/index.php][\(\mathsf{R}\) Graphical
  Manual]]\index{R Graphical Manual@\textsf{R} Graphical Manual} have
  literally thousands of graphs to peruse. [[http://www.rseek.org][\(\mathsf{R}\) Seek]] is a
  search engine based on Google specifically tailored for
  \(\mathsf{R}\) queries.

** Other Tips

It is unnecessary to retype commands repeatedly, since \(\mathsf{R}\)
remembers what you have recently entered on the command line. On the
Microsoft\(\circledR\) Windows \(\mathsf{R}\) Gui, to cycle through
the previous commands just push the \(\uparrow\) (up arrow) key. On
Emacs/ESS the command is =M-p= (which means hold down the =Alt= button
and press ``p''). More generally, the command =history()=
\index{history@\texttt{history}} will show a whole list of recently
entered commands.
- To find out what all variables are in the current work environment,
  use the commands =objects()=\index{objects@\texttt{objects}} or
  =ls()=\index{ls@\texttt{ls}}. These list all available objects in
  the workspace. If you wish to remove one or more variables, use
  =remove(var1, var2, var3)=\index{remove@\texttt{remove}}, or more
  simply use =rm(var1, var2, var3)=, and to remove all objects use
  =rm(list = ls())=.
- Another use of =scan= is when you have a long list of numbers
  (separated by spaces or on different lines) already typed somewhere
  else, say in a text file To enter all the data in one fell swoop,
  first highlight and copy the list of numbers to the Clipboard with
  =Edit= \(\triangleright\) =Copy= (or by right-clicking and selecting
  =Copy=). Next type the =x <- scan()= command in the \(\mathsf{R}\)
  console, and paste the numbers at the =1:= prompt with =Edit=
  \(\triangleright\) =Paste=. All of the numbers will automatically be
  entered into the vector =x=.
- The command =Ctrl+l= clears the display in the
  Microsoft\(\circledR\) Windows \(\mathsf{R}\) Gui. In Emacs/ESS,
  press =Ctrl+l= repeatedly to cycle point (the place where the cursor
  is) to the bottom, middle, and top of the display.
- Once you use \(\mathsf{R}\) for awhile there may be some commands
  that you wish to run automatically whenever \(\mathsf{R}\)
  starts. These commands may be saved in a file called
  =Rprofile.site=\index{Rprofile.site@\texttt{Rprofile.site}} which is
  usually in the =etc= folder, which lives in the \(\mathsf{R}\) home
  directory (which on Microsoft\(\circledR\) Windows usually is
  =C:\Program Files\R=). Alternatively, you can make a file
  =.Rprofile=\index{.Rprofile@\texttt{.Rprofile}} to be stored in the
  user's home directory, or anywhere \(\mathsf{R}\) is invoked. This
  allows for multiple configurations for different projects or
  users. See ``Customizing the Environment'' of /An Introduction to R/
  for more details.
- When exiting \(\mathsf{R}\) the user is given the option to ``save
  the workspace''. I recommend that beginners DO NOT save the
  workspace when quitting. If =Yes= is selected, then all of the
  objects and data currently in \(\mathsf{R}\)'s memory is saved in a
  file located in the working directory called
  =.RData=\index{.RData@\texttt{.RData}}. This file is then
  automatically loaded the next time \(\mathsf{R}\) starts (in which
  case \(\mathsf{R}\) will say =[previously saved workspace
  restored]=). This is a valuable feature for experienced users of
  \(\mathsf{R}\), but I find that it causes more trouble than it saves
  with beginners.

#+LaTeX: \newpage{}

** Exercises
#+LaTeX: \setcounter{thm}{0}

* Data Description                                                 :datadesc:
:PROPERTIES:
:tangle: R/datadesc.R
:CUSTOM_ID: cha-Describing-Data-Distributions
:END:

#+BEGIN_SRC R :exports none :eval never
# Chapter: Data Description
# All code released under GPL Version 3
#+END_SRC

#+LaTeX: \noindent 
In this chapter we introduce the different types of data that a
statistician is likely to encounter, and in each subsection we give
some examples of how to display the data of that particular type. Once
we see how to display data distributions, we next introduce the basic
properties of data distributions. We qualitatively explore several
data sets. Once that we have intuitive properties of data sets, we
next discuss how we may numerically measure and describe those
properties with descriptive statistics.

 *What do I want them to know?*

- different data types, such as quantitative versus qualitative,
  nominal versus ordinal, and discrete versus continuous
- basic graphical displays for assorted data types, and some of their
  (dis)advantages
- fundamental properties of data distributions, including center,
  spread, shape, and crazy observations
- methods to describe data (visually/numerically) with respect to the
  properties, and how the methods differ depending on the data type
- all of the above in the context of grouped data, and in particular,
  the concept of a factor

** Types of Data
:PROPERTIES:
:CUSTOM_ID: sec-Types-of-Data
:END: 

Loosely speaking, a datum is any piece of collected information, and a
data set is a collection of data related to each other in some way. We
will categorize data into five types and describe each in turn:

- Quantitative :: data associated with a measurement of some quantity
                  on an observational unit,
- Qualitative :: data associated with some quality or property of an
                 observational unit,
- Logical :: data which represent true or false and play an important
             role later,
- Missing :: data which should be there but are not, and
- Other types :: everything else under the sun.

In each subsection we look at some examples of the type in question
and introduce methods to display them.

*** Quantitative data
:PROPERTIES:
:CUSTOM_ID: sub-Quantitative-Data
:END:

Quantitative data are any data that measure or are associated with a
measurement of the quantity of something. They invariably assume
numerical values. Quantitative data can be further subdivided into two
categories.
- /Discrete data/ take values in a finite or countably infinite set of
  numbers, that is, all possible values could (at least in principle)
  be written down in an ordered list. Examples include: counts, number
  of arrivals, or number of successes. They are often represented by
  integers, say, 0, 1, 2, /etc/.
- /Continuous data/ take values in an interval of numbers. These are
  also known as scale data, interval data, or measurement
  data. Examples include: height, weight, length, time,
  /etc/. Continuous data are often characterized by fractions or
  decimals: 3.82, 7.0001, 4 \(\frac{5}{8}\), /etc/.

Note that the distinction between discrete and continuous data is not
always clear-cut. Sometimes it is convenient to treat data as if they
were continuous, even though strictly speaking they are not
continuous. See the examples.

#+BEGIN_exampletoo

*Annual Precipitation in US Cities.* The vector =precip=\index{Data
sets!precip@\texttt{precip}} contains average amount of rainfall (in
inches) for each of 70 cities in the United States and Puerto
Rico. Let us take a look at the data:

#+BEGIN_SRC R :exports both :results output pp  
str(precip)
#+END_SRC

#+BEGIN_SRC R :exports both :results output pp  
precip[1:4]
#+END_SRC

The output shows that =precip= is a numeric vector which has been
/named/, that is, each value has a name associated with it (which can
be set with the =names=\index{names@\texttt{names}} function). These
are quantitative continuous data.
# +END_exampletoo

#+BEGIN_exampletoo

*Lengths of Major North American Rivers.* The U.S. Geological Survey
recorded the lengths (in miles) of several rivers in North
America. They are stored in the vector =rivers=\index{Data
sets!rivers@\texttt{rivers}} in the =datasets= package \cite{datasets}
(which ships with base \(\mathsf{R}\)). See =?rivers=. Let us take a
look at the data with the =str=\index{str@\texttt{str}} function.

#+BEGIN_SRC R :exports both :results output pp  
str(rivers)
#+END_SRC

The output says that =rivers= is a numeric vector of length 141, and
the first few values are 735, 320, 325, /etc/. These data are
definitely quantitative and it appears that the measurements have been
rounded to the nearest mile. Thus, strictly speaking, these are
discrete data. But we will find it convenient later to take data like
these to be continuous for some of our statistical procedures.

# +END_exampletoo


#+BEGIN_exampletoo

*Yearly Numbers of Important Discoveries.* The vector
=discoveries=\index{Data sets!discoveries@\texttt{discoveries}}
contains numbers of “great” inventions/discoveries in each year from
1860 to 1959, as reported by the 1975 World Almanac. Let us take a
look at the data:

#+BEGIN_SRC R :exports both :results output pp  
str(discoveries)
#+END_SRC

# +END_exampletoo

The output is telling us that =discoveries= is a /time series/ (see
Section [[file:data-description::sub-other-data-types][Other Data Types]] for more) of length 100. The entries are
integers, and since they represent counts this is a good example of
discrete quantitative data. We will take a closer look in the
following sections.

*** Displaying Quantitative Data
:PROPERTIES:
:CUSTOM_ID: sub-Displaying-Quantitative-Data
:END:

One of the first things to do when confronted by quantitative data (or
any data, for that matter) is to make some sort of visual display to
gain some insight into the data's structure. There are almost as many
display types from which to choose as there are data sets to plot. We
describe some of the more popular alternatives.

**** Strip charts\index{strip chart} (also known as Dot plots)\index{dot plot| see\{strip chart\}}
:PROPERTIES:
:CUSTOM_ID: par-Strip-charts
:END:

These can be used for discrete or continuous data, and usually look
best when the data set is not too large. Along the horizontal axis is
a numerical scale above which the data values are plotted. We can do
it in \(\mathsf{R}\) with a call to the =stripchart=
\index{stripchart@\texttt{stripchart}} function. There are three
available methods.
- overplot :: plots ties covering each other. This method is good to
              display only the distinct values assumed by the data
              set.
- jitter :: adds some noise to the data in the \(y\) direction in
            which case the data values are not covered up by ties.
- stack :: plots repeated values stacked on top of one another. This
           method is best used for discrete data with a lot of ties;
           if there are no repeats then this method is identical to
           overplot.

See Figure [[fig-stripcharts]], which was produced by the following code.

#+BEGIN_SRC R :exports code :eval never
stripchart(precip, xlab="rainfall")
stripchart(rivers, method="jitter", xlab="length")
stripchart(discoveries, method="stack", xlab="number")
#+END_SRC

The leftmost graph is a strip chart of the =precip= data. The graph
shows tightly clustered values in the middle with some others falling
balanced on either side, with perhaps slightly more falling to the
left. Later we will call this a symmetric distribution, see Section
[[sub-Shape][Shape]]. The middle graph is of the =rivers= data, a vector of
length 141. There are several repeated values in the rivers data, and
if we were to use the overplot method we would lose some of them in
the display. This plot shows a what we will later call a right-skewed
shape with perhaps some extreme values on the far right of the
display. The third graph strip charts =discoveries= data which are
literally a textbook example of a right skewed distribution.

#+NAME: stripcharts
#+BEGIN_SRC R :exports none :results graphics :file ps/datadesc/stripcharts.ps
par(mfrow = c(3,1)) # 3 plots: 3 rows, 1 column
stripchart(precip, xlab="rainfall")
stripchart(rivers, method="jitter", xlab="length")
stripchart(discoveries, method="stack", xlab="number", ylim = c(0,3))
par(mfrow = c(1,1)) # back to normal
#+END_SRC

#+NAME: fig-stripcharts
#+CAPTION[Strip charts of =precip=, =rivers=, and =discoveries=]: \small Three stripcharts of three data sets.  The first graph uses the =overplot= method, the second the =jitter= method, and the third the =stack= method.
#+ATTR_LaTeX: :width 0.5\textwidth :placement [ht!]
#+RESULTS: stripcharts
[[file:ps/datadesc/stripcharts.ps]]

The =DOTplot=\index{DOTplot@\texttt{DOTplot}} function in the
=UsingR=\index{R packages!UsingR@\texttt{UsingR}} package
\cite{UsingR} is another alternative.

**** Histogram\index{Histogram}

These are typically used for continuous data. A histogram is
constructed by first deciding on a set of classes, or bins, which
partition the real line into a set of boxes into which the data values
fall. Then vertical bars are drawn over the bins with height
proportional to the number of observations that fell into the bin.

These are one of the most common summary displays, and they are often
misidentified as ``Bar Graphs'' (see below.) The scale on the \(y\)
axis can be frequency, percentage, or density (relative
frequency). The term histogram was coined by Karl Pearson in 1891, see
\cite{Miller}.

# <<exa-annual>>

*Annual Precipitation in US Cities.* We are going to take another look
at the =precip=\index{Data sets!precip@\texttt{precip}} data that we
investigated earlier. The strip chart in Figure
[[fig-stripcharts][Various-stripchart-methods]] suggested a loosely balanced distribution;
let us now look to see what a histogram says.

There are many ways to plot histograms in \(\mathsf{R}\), and one of
the easiest is with the =hist=\index{hist@\texttt{hist}} function. The
following code produces the plots in Figure [[fig-histograms]].

#+BEGIN_SRC R :exports code :eval never
hist(precip, main = "")
hist(precip, freq = FALSE, main = "")
#+END_SRC

Notice the argument \(\mathtt{main = ""}\) which suppresses the main
title from being displayed -- it would have said ``Histogram of
=precip='' otherwise. The plot on the left is a frequency histogram
(the default), and the plot on the right is a relative frequency
histogram (=freq = FALSE=).

#+NAME: histograms
#+BEGIN_SRC R :exports none :results graphics :file ps/datadesc/histograms.ps
par(mfrow = c(1,2))
hist(precip, main = "")
hist(precip, freq = FALSE, main = "")
par(mfrow = c(1,1))
#+END_SRC

#+NAME: fig-histograms
#+CAPTION: \small (Relative) frequency histograms of the =precip= data
#+ATTR_LaTeX: :width 0.5\textwidth :placement [ht!]
#+RESULTS: histograms
[[file:ps/datadesc/histograms.ps]]

Please mind the biggest weakness of histograms: the graph obtained
strongly depends on the bins chosen. Choose another set of bins, and
you will get a different histogram. Moreover, there are not any
definitive criteria by which bins should be defined; the best choice
for a given data set is the one which illuminates the data set's
underlying structure (if any). Luckily for us there are algorithms to
automatically choose bins that are likely to display well, and more
often than not the default bins do a good job. This is not always the
case, however, and a responsible statistician will investigate many
bin choices to test the stability of the display.

Recall that the strip chart in Figure [[fig-stripcharts][Various-stripchart-methods]]
suggested a relatively balanced shape to the =precip= data
distribution. Watch what happens when we change the bins slightly
(with the =breaks= argument to =hist=). See Figure [[fig-histograms-bins][histograms-bins]]
which was produced by the following code.

#+NAME: histograms-bins
#+BEGIN_SRC R :exports none :results graphics silent :file ps/datadesc/histograms-bins.ps
par(mfrow = c(1,3))
hist(precip, breaks = 10, main = "")
hist(precip, breaks = 25, main = "")
hist(precip, breaks = 50, main = "")
par(mfrow = c(1,1))
#+END_SRC

#+NAME: fig-histograms-bins
#+CAPTION: \small More histograms of the =precip= data
#+ATTR_LaTeX: :width 0.5\textwidth :placement [ht!]
#+RESULTS: histograms-bins
[[file:ps/datadesc/histograms-bins.ps]]

The leftmost graph (with =breaks = 10=) shows that the distribution is
not balanced at all. There are two humps: a big one in the middle and
a smaller one to the left. Graphs like this often indicate some
underlying group structure to the data; we could now investigate
whether the cities for which rainfall was measured were similar in
some way, with respect to geographic region, for example.

The rightmost graph in Figure [[fig-histograms-bins][histograms-bins]] shows what happens when
the number of bins is too large: the histogram is too grainy and hides
the rounded appearance of the earlier histograms. If we were to
continue increasing the number of bins we would eventually get all
observed bins to have exactly one element, which is nothing more than
a glorified strip chart.

**** Stem-and-leaf displays (more to be said in Section [[sec-Exploratory-Data-Analysis][Exploratory Data Analysis]])
Stem-and-leaf displays (also known as stemplots) have two basic parts:
/stems/ and /leaves/. The final digit of the data values is taken to
be a /leaf/, and the leading digit(s) is (are) taken to be /stems/. We
draw a vertical line, and to the left of the line we list the
stems. To the right of the line, we list the leaves beside their
corresponding stem. There will typically be several leaves for each
stem, in which case the leaves accumulate to the right. It is
sometimes necessary to round the data values, especially for larger
data sets.

# <<exa-ukdriverdeaths-first>>
=UKDriverDeaths=\index{Data
sets!UKDriverDeaths@\texttt{UKDriverDeaths}} is a time series that
contains the total car drivers killed or seriously injured in Great
Britain monthly from Jan 1969 to Dec 1984. See
=?UKDriverDeaths=. Compulsory seat belt use was introduced on January
31, 1983. We construct a stem and leaf diagram in \(\mathsf{R}\) with
the =stem.leaf=\index{stem.leaf@\texttt{stem.leaf}} function from the
=aplpack=\index{R packages@\textsf{R}
packages!aplpack@\texttt{aplpack}} package\cite{aplpack}.

#+BEGIN_SRC R :exports both :results output pp  
stem.leaf(UKDriverDeaths, depth = FALSE)
#+END_SRC

The display shows a more or less balanced mound-shaped distribution,
with one or maybe two humps, a big one and a smaller one just to its
right. Note that the data have been rounded to the tens place so that
each datum gets only one leaf to the right of the dividing line.

Notice that the \texttt{depth}s\index{depths} have been suppressed. To
learn more about this option and many others, see Section [[sec-Exploratory-Data-Analysis][Exploratory
Data Analysis]]. Unlike a histogram, the original data values may be
recovered from the stem-and-leaf display -- modulo the rounding --
that is, starting from the top and working down we can read off the
data values 1050, 1070, 1110, 1130, and so forth.

**** Index plots

Done with the =plot=\index{plot@\texttt{plot}} function. These are
good for plotting data which are ordered, for example, when the data
are measured over time. That is, the first observation was measured at
time 1, the second at time 2, /etc/. It is a two dimensional plot, in
which the index (or time) is the \(x\) variable and the measured value
is the \(y\) variable. There are several plotting methods for index
plots, and we mention two of them:

- spikes :: draws a vertical line from the \(x\)-axis to the observation height.
- points :: plots a simple point at the observation height.

*Level of Lake Huron 1875-1972.* Brockwell and Davis
\cite{Brockwell1991} give the annual measurements of the level (in
feet) of Lake Huron from 1875--1972. The data are stored in the time
series =LakeHuron=\index{Data sets!LakeHuron@\texttt{LakeHuron}}. See
=?LakeHuron=. Figure [[fig-indpl-lakehuron][indpl-lakehuron]] was produced with the following
code:

Here is how to do it with base \(\mathsf{R}\).

#+BEGIN_SRC R :exports code :eval never
plot(LakeHuron)
plot(LakeHuron, type = "p")
plot(LakeHuron, type = "h")
#+END_SRC

The plots show an overall decreasing trend to the observations, and
there appears to be some seasonal variation that increases over time.

#+NAME: indpl-lakehuron
#+BEGIN_SRC R :exports none :results graphics silent :file ps/datadesc/indpl-lakehuron.ps
par(mfrow = c(3,1))
plot(LakeHuron)
plot(LakeHuron, type = "p")
plot(LakeHuron, type = "h")
par(mfrow = c(1,1))
#+END_SRC

#+NAME: fig-indpl-lakehuron
#+CAPTION: \small Index plots of the =LakeHuron= data
#+ATTR_LaTeX: :width 0.5\textwidth :placement [ht!]
#+RESULTS: indpl-lakehuron
[[file:ps/datadesc/indpl-lakehuron.ps]]

**** Density estimates						       :TODO:

The default method uses a Gaussian kernel density estimate.

#+BEGIN_SRC R :eval never
# The Old Faithful geyser data
d <- density(faithful$eruptions, bw = "sj")
d
plot(d)
hist(precip, freq = FALSE)
lines(density(precip))
#+END_SRC

*** Qualitative Data, Categorical Data, and Factors
:PROPERTIES:
:CUSTOM_ID: sub-Qualitative-Data
:END:

Qualitative data are simply any type of data that are not numerical,
or do not represent numerical quantities. Examples of qualitative
variables include a subject's name, gender, race/ethnicity, political
party, socioeconomic status, class rank, driver's license number, and
social security number (SSN).

Please bear in mind that some data /look/ to be quantitative but are
/not/, because they do not represent numerical quantities and do not
obey mathematical rules. For example, a person's shoe size is
typically written with numbers: 8, or 9, or 12, or
\(12\,\frac{1}{2}\). Shoe size is not quantitative, however, because
if we take a size 8 and combine with a size 9 we do not get a size 17.

Some qualitative data serve merely to /identify/ the observation (such
a subject's name, driver's license number, or SSN). This type of data
does not usually play much of a role in statistics. But other
qualitative variables serve to /subdivide/ the data set into
categories; we call these /factors/. In the above examples, gender,
race, political party, and socioeconomic status would be considered
factors (shoe size would be another one). The possible values of a
factor are called its /levels/. For instance, the factor /gender/
would have two levels, namely, male and female. Socioeconomic status
typically has three levels: high, middle, and low.

Factors may be of two types: /nominal/\index{nominal data} and
/ordinal/\index{ordinal data}. Nominal factors have levels that
correspond to names of the categories, with no implied
ordering. Examples of nominal factors would be hair color, gender,
race, or political party. There is no natural ordering to ``Democrat''
and ``Republican''; the categories are just names associated with
different groups of people.

In contrast, ordinal factors have some sort of ordered structure to
the underlying factor levels. For instance, socioeconomic status would
be an ordinal categorical variable because the levels correspond to
ranks associated with income, education, and occupation. Another
example of ordinal categorical data would be class rank.

Factors have special status in \(\mathsf{R}\). They are represented
internally by numbers, but even when they are written numerically
their values do not convey any numeric meaning or obey any
mathematical rules (that is, Stage III cancer is not Stage I cancer +
Stage II cancer).

# +BEGIN_exampletoo

The =state.abb=\index{Data sets!state.abb@\texttt{state.abb}} vector
gives the two letter postal abbreviations for all 50 states.

#+BEGIN_SRC R :exports both :results output pp  
str(state.abb)
#+END_SRC

These would be ID data. The =state.name=\index{Data
sets!state.name@\texttt{state.name}} vector lists all of the complete
names and those data would also be ID.
# +END_exampletoo


# +BEGIN_exampletoo

*U.S. State Facts and Features.* The U.S. Department of Commerce of
the U.S. Census Bureau releases all sorts of information in the
/Statistical Abstract of the United States/, and the
=state.region=\index{Data sets!state.region@\texttt{state.region}}
data lists each of the 50 states and the region to which it belongs,
be it Northeast, South, North Central, or West. See =?state.region=.

#+BEGIN_SRC R :exports both :results output pp  
str(state.region)
state.region[1:5]
#+END_SRC

The =str=\index{str@\texttt{str}} output shows that =state.region= is
already stored internally as a factor and it lists a couple of the
factor levels. To see all of the levels we printed the first five
entries of the vector in the second line.
# +END_exampletoo


*** Displaying Qualitative Data
:PROPERTIES:
:CUSTOM_ID: sub-Displaying-Qualitative-Data
:END:

**** Tables
:PROPERTIES:
:CUSTOM_ID: par-Tables
:END:

One of the best ways to summarize qualitative data is with a table of
the data values. We may count frequencies with the =table= function or
list proportions with the =prop.table=
\index{prop.table@\texttt{prop.table}} function (whose input is a
frequency table). In the \(\mathsf{R}\) Commander you can do it with
=Statistics= \(\triangleright\) =Frequency Distribution...=
Alternatively, to look at tables for all factors in the =Active data
set=\index{Active data set@\texttt{Active data set}} you can do
=Statistics= \(\triangleright\) =Summaries= \(\triangleright\) =Active
Dataset=.

#+BEGIN_SRC R :exports code :results silent 
Tbl <- table(state.division)
#+END_SRC

#+BEGIN_SRC R :exports both :results output pp  
Tbl
#+END_SRC

#+BEGIN_SRC R :exports both :results output pp  
Tbl/sum(Tbl)      # relative frequencies
#+END_SRC

#+BEGIN_SRC R :exports both :results output pp  
prop.table(Tbl)   # same thing
#+END_SRC

**** Bar Graphs
:PROPERTIES:
:CUSTOM_ID: par-Bar-Graphs
:END:

A bar graph is the analogue of a histogram for categorical data. A bar
is displayed for each level of a factor, with the heights of the bars
proportional to the frequencies of observations falling in the
respective categories. A disadvantage of bar graphs is that the levels
are ordered alphabetically (by default), which may sometimes obscure
patterns in the display.

# +BEGIN_exampletoo

*U.S. State Facts and Features.* The =state.region= data lists each of
the 50 states and the region to which it belongs, be it Northeast,
South, North Central, or West. See =?state.region=. It is already
stored internally as a factor. We make a bar graph with the
=barplot=\index{barplot@\texttt{barplot}} function:

#+BEGIN_SRC R :exports code :eval never
barplot(table(state.region), cex.names = 0.50)
barplot(prop.table(table(state.region)), cex.names = 0.50)
#+END_SRC

See Figure [[fig-bar-gr-stateregion][bar-gr-stateregion]]. The display on the left is a frequency
bar graph because the \(y\) axis shows counts, while the display on
the left is a relative frequency bar graph. The only difference
between the two is the scale. Looking at the graph we see that the
majority of the fifty states are in the South, followed by West, North
Central, and finally Northeast. Over 30% of the states are in the
South.

Notice the =cex.names=\index{cex.names@\texttt{cex.names}} argument that we used, above. It shrinks the names on the \(x\) axis by 50% which makes them easier to read. See =?par=\index{par@\texttt{par}} for a detailed list of additional plot parameters.

#+NAME: bar-gr-stateregion
#+BEGIN_SRC R :exports none :results graphics silent :file ps/datadesc/bar-gr-stateregion.ps
par(mfrow = c(1,2)) # 2 plots: 1 row, 2 columns
barplot(table(state.region), cex.names = 0.50)
barplot(prop.table(table(state.region)), cex.names = 0.50)
par(mfrow = c(1,1)) # back to normal
#+END_SRC

#+NAME: fig-bar-gr-stateregion
#+CAPTION[Bar graphs of the =state.region= data]: \small The left graph is a frequency barplot made with =table= and the right is a relative frequency barplot made with =prop.table=.
#+ATTR_LaTeX: :width 0.5\textwidth :placement [ht!]
#+RESULTS: bar-gr-stateregion
[[file:ps/datadesc/fig-bar-gr-stateregion.ps]]

# +END_exampletoo

**** Pareto Diagrams
:PROPERTIES:
:CUSTOM_ID: par-Pareto-Diagrams
:END:

A pareto diagram is a lot like a bar graph except the bars are
rearranged such that they decrease in height going from left to
right. The rearrangement is handy because it can visually reveal
structure (if any) in how fast the bars decrease -- this is much more
difficult when the bars are jumbled.

# +BEGIN_exampletoo

*U.S. State Facts and Features.* The =state.division=\index{Data
sets!state.division@\texttt{state.division}} data record the division
(New England, Middle Atlantic, South Atlantic, East South Central,
West South Central, East North Central, West North Central, Mountain,
and Pacific) of the fifty states. We can make a pareto diagram with
either the =RcmdrPlugin.IPSUR=\index{R packages@\textsf{R}
packages!RcmdrPlugin.IPSUR@\texttt{RcmdrPlugin.IPSUR}} package
\cite{RcmdrPlugin.IPSUR} or with the
=pareto.chart=\index{pareto.chart@\texttt{pareto.chart}} function from
the =qcc=\index{R packages@\textsf{R} packages!qcc@\texttt{qcc}}
package \cite{qcc}. See Figure [[fig-Pareto-chart][Pareto-chart]]. The code follows.

#+NAME: Pareto-chart
#+BEGIN_SRC R :exports code :exports none :results graphics silent :file ps/datadesc/Pareto-chart.ps
pareto.chart(table(state.division), ylab="Frequency")
#+END_SRC

#+NAME: fig-Pareto-chart
#+CAPTION: \small Pareto chart of the =state.division= data.
#+ATTR_LaTeX: :width 0.5\textwidth :placement [ht!]
#+RESULTS: Pareto-chart
[[file:ps/datadesc/fig-bar-gr-stateregion.ps]]

# +END_exampletoo

**** Dot Charts
:PROPERTIES:
:CUSTOM_ID: par-Dotcharts
:END:

These are a lot like a bar graph that has been turned on its side with
the bars replaced by dots on horizontal lines. They do not convey any
more (or less) information than the associated bar graph, but the
strength lies in the economy of the display. Dot charts are so compact
that it is easy to graph very complicated multi-variable interactions
together in one graph. See Section [[sec-Comparing-Data-Sets][Comparing Data Sets]]. We will give
an example here using the same data as above for comparison. The graph
was produced by the following code.

# +BEGIN_exampletoo

#+NAME: dot-charts
#+BEGIN_SRC R :exports code :results graphics :file ps/datadesc/dot-charts.ps
x <- table(state.region)
dotchart(as.vector(x), labels = names(x))
#+END_SRC

#+NAME: fig-dot-charts
#+CAPTION: \small Dot chart of the \texttt{state.region} data.
#+ATTR_LaTeX: :width 0.5\textwidth :placement [ht!]
#+RESULTS: dot-charts
[[file:ps/datadesc/dot-charts.ps]]

See Figure [[fig-dot-charts][dot-charts]]. Compare it to Figure [[fig-bar-gr-stateregion][bar-gr-stateregion]].

# +END_exampletoo

**** Pie Graphs
:PROPERTIES:
:CUSTOM_ID: par-Pie-Graphs
:END:

These can be done with \(\mathsf{R}\) and the \(\mathsf{R}\)
Commander, but they fallen out of favor in recent years because
researchers have determined that while the human eye is good at
judging linear measures, it is notoriously bad at judging relative
areas (such as those displayed by a pie graph). Pie charts are
consequently a very bad way of displaying information. A bar chart or
dot chart is a preferable way of displaying qualitative data. See
=?pie=\index{pie@\texttt{pie}} for more information.

We are not going to do any examples of a pie graph and discourage
their use elsewhere.

*** Logical Data
:PROPERTIES:
:CUSTOM_ID: sub-Logical-Data
:END:

There is another type of information recognized by \(\mathsf{R}\)
which does not fall into the above categories. The value is either
=TRUE= or =FALSE= (note that equivalently you can use =1 = TRUE=, =0 =
FALSE=). Here is an example of a logical vector:

#+BEGIN_SRC R :exports both :results output pp  
x <- 5:9
y <- (x < 7.3)
y
#+END_SRC

Many functions in \(\mathsf{R}\) have options that the user may or may
not want to activate in the function call. For example, the
=stem.leaf= function has the =depths= argument which is =TRUE= by
default. We saw in Section [[sub-Quantitative-Data][Quantitative Data]] how to turn the option
off, simply enter =stem.leaf(x, depths = FALSE)= and they will not be
shown on the display.

We can swap =TRUE= with =FALSE= with the exclamation point =!=.

#+BEGIN_SRC R :exports both :results output pp  
!y
#+END_SRC

*** Missing Data
:PROPERTIES:
:CUSTOM_ID: sub-Missing-Data
:END:

Missing data are a persistent and prevalent problem in many
statistical analyses, especially those associated with the social
sciences. \(\mathsf{R}\) reserves the special symbol =NA= to
representing missing data.

Ordinary arithmetic with =NA= values give =NA='s (addition,
subtraction, /etc/.) and applying a function to a vector that has an
=NA= in it will usually give an =NA=.

#+BEGIN_SRC R :exports both :results output pp  
x <- c(3, 7, NA, 4, 7)
y <- c(5, NA, 1, 2, 2)
x + y
#+END_SRC

Some functions have a =na.rm= argument which when =TRUE= will ignore
missing data as if they were not there (such as =mean=, =var=, =sd=,
=IQR=, =mad=, ...).

#+BEGIN_SRC R :exports both :results output pp  
sum(x)
sum(x, na.rm = TRUE)
#+END_SRC

Other functions do not have a =na.rm= argument and will return =NA= or
an error if the argument has \texttt{NA}s. In those cases we can find
the locations of any \texttt{NA}s with the =is.na= function and remove
those cases with the =[]= operator.

#+BEGIN_SRC R :exports both :results output pp  
is.na(x)
z <- x[!is.na(x)]
sum(z)
#+END_SRC

The analogue of =is.na= for rectangular data sets (or data frames) is
the =complete.cases= function. See Appendix [[sec-Editing-Data-Sets][Editing Data Sets]].

*** Other Data Types
:PROPERTIES:
:CUSTOM_ID: sub-other-data-types
:END:

** Features of Data Distributions
:PROPERTIES:
:CUSTOM_ID: sec-features-of-data
:END:

Given that the data have been appropriately displayed, the next step
is to try to identify salient features represented in the graph. The
acronym to remember is /C/-enter, /U/-nusual features, /S/-pread, and
/S/-hape. (CUSS).

*** Center
:PROPERTIES:
:CUSTOM_ID: sub-Center
:END:

One of the most basic features of a data set is its center. Loosely
speaking, the center of a data set is associated with a number that
represents a middle or general tendency of the data. Of course, there
are usually several values that would serve as a center, and our later
tasks will be focused on choosing an appropriate one for the data at
hand. Judging from the histogram that we saw in Figure
[[fig-histograms-bins][histograms-bins]], a measure of center would be about \(
SRC_R{round(mean(precip))} \).

*** Spread
:PROPERTIES:
:CUSTOM_ID: sub-Spread
:END:

The spread of a data set is associated with its variability; data sets
with a large spread tend to cover a large interval of values, while
data sets with small spread tend to cluster tightly around a central
value.

*** Shape
:PROPERTIES:
:CUSTOM_ID: sub-Shape
:END:

When we speak of the /shape/ of a data set, we are usually referring
to the shape exhibited by an associated graphical display, such as a
histogram. The shape can tell us a lot about any underlying structure
to the data, and can help us decide which statistical procedure we
should use to analyze them.

**** Symmetry and Skewness

A distribution is said to be /right-skewed/ (or /positively skewed/)
if the right tail seems to be stretched from the center. A
/left-skewed/ (or /negatively skewed/) distribution is stretched to
the left side. A symmetric distribution has a graph that is balanced
about its center, in the sense that half of the graph may be reflected
about a central line of symmetry to match the other half.

We have already encountered skewed distributions: both the discoveries
data in Figure [[fig-stripcharts][stripcharts]] and the =precip= data in Figure
[[fig-histograms-bins][histograms-bins]] appear right-skewed. The =UKDriverDeaths= data in
Example [[exa-ukdriverdeaths-first][UK Driver Deaths]] is relatively symmetric (but note the one
extreme value 2654 identified at the bottom of the stem-and-leaf
display).

**** Kurtosis

Another component to the shape of a distribution is how ``peaked'' it
is. Some distributions tend to have a flat shape with thin
tails. These are called /platykurtic/, and an example of a platykurtic
distribution is the uniform distribution; see Section [[sec-The-Continuous-Uniform][Continuous
Uniform]]. On the other end of the spectrum are distributions with a
steep peak, or spike, accompanied by heavy tails; these are called
/leptokurtic/. Examples of leptokurtic distributions are the Laplace
distribution and the logistic distribution. See Section [[sec-Other-Continuous-Distributions][Other
Continuous Distributions]]. In between are distributions (called
/mesokurtic/) with a rounded peak and moderately sized tails. The
standard example of a mesokurtic distribution is the famous
bell-shaped curve, also known as the Gaussian, or normal,
distribution, and the binomial distribution can be mesokurtic for
specific choices of \(p\). See Sections [[sec-binom-dist][Binomial Distribution]] and [[sec-The-Normal-Distribution][The
Normal Distribution]].

*** Clusters and Gaps
:PROPERTIES:
:CUSTOM_ID: sub-clusters-and-gaps
:END:

Clusters or gaps are sometimes observed in quantitative data
distributions. They indicate clumping of the data about distinct
values, and gaps may exist between clusters. Clusters often suggest an
underlying grouping to the data. For example, take a look at the
=faithful= data which contains the duration of =eruptions= and the
=waiting= time between eruptions of the Old Faithful geyser in
Yellowstone National Park. Do not be frightened by the complicated
information at the left of the display for now; we will learn how to
interpret it in Section [[sec-Exploratory-Data-Analysis][Exploratory Data Analysis]].

# <<exa-stemleaf-multiple-lines-stem>>
#+BEGIN_SRC R :exports both :results output pp
with(faithful, stem.leaf(eruptions))
#+END_SRC

There are definitely two clusters of data here; an upper cluster and a
lower cluster.

*** Extreme Observations and other Unusual Features
:PROPERTIES:
:CUSTOM_ID: sub-Extreme-Observations-and
:END:

Extreme observations fall far from the rest of the data. Such
observations are troublesome to many statistical procedures; they
cause exaggerated estimates and instability. It is important to
identify extreme observations and examine the source of the data more
closely. There are many possible reasons underlying an extreme
observation:
- *Maybe the value is a typographical error.* Especially with large
  data sets becoming more prevalent, many of which being recorded by
  hand, mistakes are a common problem. After closer scrutiny, these
  can often be fixed.
- *Maybe the observation was not meant for the study*, because it does
  not belong to the population of interest. For example, in medical
  research some subjects may have relevant complications in their
  genealogical history that would rule out their participation in the
  experiment. Or when a manufacturing company investigates the
  properties of one of its devices, perhaps a particular product is
  malfunctioning and is not representative of the majority of the
  items.
- *Maybe it indicates a deeper trend or phenomenon.* Many of the most
  influential scientific discoveries were made when the investigator
  noticed an unexpected result, a value that was not predicted by the
  classical theory. Albert Einstein, Louis Pasteur, and others built
  their careers on exactly this circumstance.

** Descriptive Statistics
:PROPERTIES:
:CUSTOM_ID: sec-Descriptive-Statistics
:END:

One of my favorite professors would repeatedly harp, ``You cannot do
statistics without data.''

*What do I want them to know?*
- The fundamental data types we encounter most often, how to classify
  given data into a likely type, and that sometimes the distinction is
  blurry.\

*** Frequencies and Relative Frequencies
:PROPERTIES:
:CUSTOM_ID: sub-Frequencies-and-Relative
:END:

These are used for categorical data. The idea is that there are a
number of different categories, and we would like to get some idea
about how the categories are represented in the population.

*** Measures of Center
:PROPERTIES:
:CUSTOM_ID: sub-Measures-of-Center
:END:

The /sample mean/ is denoted \(\overline{x}\) (read ``\(x\)-bar'') and
is simply the arithmetic average of the observations:
\begin{equation} 
\overline{x}=\frac{x_{1}+x_{2}+\cdots+x_{n}}{n}=\frac{1}{n}\sum_{i=1}^{n}x_{i}.
\end{equation}
- Good: natural, easy to compute, has nice mathematical properties
- Bad: sensitive to extreme values

It is appropriate for use with data sets that are not highly skewed
without extreme observations.

The /sample median/ is another popular measure of center and is
denoted \(\tilde{x}\). To calculate its value, first sort the data
into an increasing sequence of numbers. If the data set has an odd
number of observations then \(\tilde{x}\) is the value of the middle
observation, which lies in position \((n+1)/2\); otherwise, there are
two middle observations and \(\tilde{x}\) is the average of those
middle values.
- Good: resistant to extreme values, easy to describe
- Bad: not as mathematically tractable, need to sort the data to calculate

One desirable property of the sample median is that it is /resistant/
to extreme observations, in the sense that the value of \(\tilde{x}\)
depends only on those data values in the middle, and is quite
unaffected by the actual values of the outer observations in the
ordered list. The same cannot be said for the sample mean. Any
significant changes in the magnitude of an observation \(x_{k}\)
results in a corresponding change in the value of the
mean. Consequently, the sample mean is said to be /sensitive/ to
extreme observations.

The /trimmed mean/ is a measure designed to address the sensitivity of
the sample mean to extreme observations. The idea is to ``trim'' a
fraction (less than 1/2) of the observations off each end of the
ordered list, and then calculate the sample mean of what remains. We
will denote it by \(\overline{x}_{t=0.05}\).

- Good: resistant to extreme values, shares nice statistical properties
- Bad: need to sort the data

**** How to do it with \(\mathsf{R}\)

- You can calculate frequencies or relative frequencies with the
  =table= function, and relative frequencies with
  =prop.table(table())=.
- You can calculate the sample mean of a data vector =x= with the
  command =mean(x)=.
- You can calculate the sample median of =x= with the command =median(x)=.
- You can calculate the trimmed mean with the =trim= argument;
  =mean(x, trim = 0.05)=.

*** Order Statistics and the Sample Quantiles
:PROPERTIES:
:CUSTOM_ID: sub-Order-Statistics-and
:END:

A common first step in an analysis of a data set is to sort the
values. Given a data set \(x_{1}\), \(x_{2}\), ..., \(x_{n}\), we may
sort the values to obtain an increasing sequence
\begin{equation} 
x_{(1)}\leq x_{(2)}\leq x_{(3)}\leq\cdots\leq x_{(n)}
\end{equation}
and the resulting values are called the /order statistics/. The
\(k^{\mathrm{th}}\) entry in the list, \(x_{(k)}\), is the
\(k^{\mathrm{th}}\) order statistic, and approximately \(100(k/n)\)%
of the observations fall below \(x_{(k)}\). The order statistics give
an indication of the shape of the data distribution, in the sense that
a person can look at the order statistics and have an idea about where
the data are concentrated, and where they are sparse.

The /sample quantiles/ are related to the order
statistics. Unfortunately, there is not a universally accepted
definition of them. Indeed, \(\mathsf{R}\) is equipped to calculate
quantiles using nine distinct definitions! We will describe the
default method (=type = 7=), but the interested reader can see the
details for the other methods with =?quantile=.

Suppose the data set has \(n\) observations. Find the sample quantile
of order \(p\) (\(0<p<1\)), denoted \(\tilde{q}_{p}\) , as follows:

- First step: :: sort the data to obtain the order statistics
                 \(x_{(1)}\), \(x_{(2)}\), ...,\(x_{(n)}\).
- Second step: :: calculate \((n-1)p+1\) and write it in the form
                  \(k.d\), where \(k\) is an integer and \(d\) is a
                  decimal.
- Third step: :: The sample quantile \(\tilde{q}_{p}\) is
   \begin{equation}
      \tilde{q}_{p}=x_{(k)}+d(x_{(k+1)}-x_{(k)}).
   \end{equation}

The interpretation of \(\tilde{q}_{p}\) is that approximately \(100p\)
% of the data fall below the value \(\tilde{q}_{p}\).

Keep in mind that there is not a unique definition of percentiles,
quartiles, /etc/. Open a different book, and you'll find a different
definition. The difference is small and seldom plays a role except in
small data sets with repeated values. In fact, most people do not even
notice in common use.

Clearly, the most popular sample quantile is \(\tilde{q}_{0.50}\),
also known as the sample median, \(\tilde{x}\). The closest runners-up
are the /first quartile/ \(\tilde{q}_{0.25}\) and the /third quartile/
\(\tilde{q}_{0.75}\) (the /second quartile/ is the median).

**** How to do it with \(\mathsf{R}\)

*At the command prompt* We can find the order statistics of a data set
stored in a vector =x= with the command =sort(x)=.

We can calculate the sample quantiles of any order \(p\) where
\(0<p<1\) for a data set stored in a data vector =x= with the
=quantile= function, for instance, the command =quantile(x, probs =
c(0, 0.25, 0.37))= will return the smallest observation, the first
quartile, \(\tilde{q}_{0.25}\), and the 37th sample quantile,
\(\tilde{q}_{0.37}\). For \(\tilde{q}_{p}\) simply change the values
in the =probs= argument to the value \(p\).


*With the R Commander* we can find the order statistics of a variable
in the =Active data set= by doing =Data= \(\triangleright\) =Manage
variables in Active data set...= \(\triangleright\) =Compute new
variable...= In the =Expression to compute= dialog simply type
=sort(varname)=, where =varname= is the variable that it is desired to
sort.

In =Rcmdr=, we can calculate the sample quantiles for a particular
variable with the sequence =Statistics= \(\triangleright\) =Summaries=
\(\triangleright\) =Numerical Summaries...= We can automatically
calculate the quartiles for all variables in the =Active data set=
with the sequence =Statistics= \(\triangleright\) =Summaries=
\(\triangleright\) =Active Dataset=.

*** Measures of Spread
:PROPERTIES:
:CUSTOM_ID: sub-Measures-of-Spread
:END:

**** Sample Variance and Standard Deviation

The /sample variance/ is denoted \(s^{2}\) and is calculated with the
formula
\begin{equation}
s^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}.
\end{equation}
The /sample standard deviation/ is \(s=\sqrt{s^{2}}\). Intuitively,
the sample variance is approximately the average squared distance of
the observations from the sample mean. The sample standard deviation
is used to scale the estimate back to the measurement units of the
original data.
- Good: tractable, has nice mathematical/statistical properties
- Bad: sensitive to extreme values
We will spend a lot of time with the variance and standard deviation
in the coming chapters. In the meantime, the following two rules give
some meaning to the standard deviation, in that there are bounds on
how much of the data can fall past a certain distance from the mean.

#+begin_fact
Chebychev's Rule: The proportion of observations within \(k\) standard
deviations of the mean is at least \(1-1/k^{2}\), /i.e./, at least
75%, 89%, and 94% of the data are within 2, 3, and 4 standard
deviations of the mean, respectively.
#+end_fact

Note that Chebychev's Rule does not say anything about when \(k=1\),
because \(1-1/1^{2}=0\), which states that at least 0% of the
observations are within one standard deviation of the mean (which is
not saying much).

Chebychev's Rule applies to any data distribution, /any/ list of
numbers, no matter where it came from or what the histogram looks
like. The price for such generality is that the bounds are not very
tight; if we know more about how the data are shaped then we can say
more about how much of the data can fall a given distance from the
mean.

#+begin_fact
# <<fac-Empirical-Rule>>
Empirical Rule: If data follow a bell-shaped curve, then approximately
68%, 95%, and 99.7% of the data are within 1, 2, and 3 standard
deviations of the mean, respectively.
#+end_fact

**** Interquartile Range

Just as the sample mean is sensitive to extreme values, so the
associated measure of spread is similarly sensitive to
extremes. Further, the problem is exacerbated by the fact that the
extreme distances are squared. We know that the sample quartiles are
resistant to extremes, and a measure of spread associated with them is
the /interquartile range/ (\(IQR\)) defined by
\(IQR=q_{0.75}-q_{0.25}\).
- Good: stable, resistant to outliers, robust to nonnormality, easy to
  explain
- Bad: not as tractable, need to sort the data, only involves the
  middle 50% of the data.

**** Median Absolute Deviation

A measure even more robust than the \(IQR\) is the /median absolute
deviation/ (\(MAD\)). To calculate it we first get the median
\(\widetilde{x}\), next the /absolute deviations/
\(|x_{1}-\tilde{x}|\), \(|x_{2}-\tilde{x}|\), ...,
\(|x_{n}-\tilde{x}|\), and the \(MAD\) is proportional to the median
of those deviations:
\begin{equation}
MAD\propto\mbox{median}(|x_{1}-\tilde{x}|,\ |x_{2}-\tilde{x}|,\ldots,|x_{n}-\tilde{x}|).
\end{equation}
That is, the
\(MAD=c\cdot\mbox{median}(|x_{1}-\tilde{x}|,\ |x_{2}-\tilde{x}|,\ldots,|x_{n}-\tilde{x}|)\),
where \(c\) is a constant chosen so that the \(MAD\) has nice
properties. The value of \(c\) in \(\mathsf{R}\) is by default
\(c=1.4286\). This value is chosen to ensure that the estimator of
\(\sigma\) is correct, on the average, under suitable sampling
assumptions (see Section [[sec-Point-Estimation-1][Point Estimation 1]]).
- Good: stable, very robust, even more so than the \(IQR\).
- Bad: not tractable, not well known and less easy to explain.

**** Comparing Apples to Apples

We have seen three different measures of spread which, for a given
data set, will give three different answers. Which one should we use?
It depends on the data set. If the data are well behaved, with an
approximate bell-shaped distribution, then the sample mean and sample
standard deviation are natural choices with nice mathematical
properties. However, if the data have an unusual or skewed shape with
several extreme values, perhaps the more resistant choices among the
\(IQR\) or \(MAD\) would be more appropriate.

However, once we are looking at the three numbers it is important to
understand that the estimators are not all measuring the same
quantity, on the average. In particular, it can be shown that when the
data follow an approximately bell-shaped distribution, then on the
average, the sample standard deviation \(s\) and the \(MAD\) will be
the approximately the same value, namely, \(\sigma\), but the \(IQR\)
will be on the average 1.349 times larger than \(s\) and the
\(MAD\). See [[cha-Sampling-Distributions][Sampling Distributions]] for more details.

**** How to do it with \(\mathsf{R}\)

*At the command prompt* we may compute the sample range with
=range(x)= and the sample variance with =var(x)=, where =x= is a
numeric vector. The sample standard deviation is =sqrt(var(x))= or
just =sd(x)=. The \(IQR\) is =IQR(x)= and the median absolute
deviation is =mad(x)=.

*With the R Commander* we can calculate the sample standard deviation
with the =Statistics= \(\triangleright\) =Summaries=
\(\triangleright\) =Numerical Summaries...=
combination. \(\mathsf{R}\) Commander does not calculate the \(IQR\)
or \(MAD\) in any of the menu selections, by default.

*** Measures of Shape
:PROPERTIES:
:CUSTOM_ID: sub-Measures-of-Shape
:END: 

**** Sample Skewness

The /sample skewness/, denoted by \(g_{1}\), is defined by the formula
\begin{equation}
g_{1}=\frac{1}{n}\frac{\sum_{i=1}^{n}(x_{i}-\overline{x})^{3}}{s^{3}}.
\end{equation}
The sample skewness can be any value \(-\infty<g_{1}<\infty\). The
sign of \(g_{1}\) indicates the direction of skewness of the
distribution. Samples that have \(g_{1}>0\) indicate right-skewed
distributions (or positively skewed), and samples with \(g_{1}<0\)
indicate left-skewed distributions (or negatively skewed). Values of
\(g_{1}\) near zero indicate a symmetric distribution. These are not
hard and fast rules, however. The value of \(g_{1}\) is subject to
sampling variability and thus only provides a suggestion to the
skewness of the underlying distribution.

We still need to know how big is ``big'', that is, how do we judge
whether an observed value of \(g_{1}\) is far enough away from zero
for the data set to be considered skewed to the right or left? A good
rule of thumb is that data sets with skewness larger than
\(2\sqrt{6/n}\) in magnitude are substantially skewed, in the
direction of the sign of \(g_{1}\). See Tabachnick & Fidell
\cite{Tabachnick2006} for details.

**** Sample Excess Kurtosis

The /sample excess kurtosis/, denoted by \(g_{2}\), is given by the
formula
\begin{equation}
g_{2}=\frac{1}{n}\frac{\sum_{i=1}^{n}(x_{i}-\overline{x})^{4}}{s^{4}}-3.
\end{equation}
The sample excess kurtosis takes values \(-2\leq g_{2}<\infty\). The
subtraction of 3 may seem mysterious but it is done so that mound
shaped samples have values of \(g_{2}\) near zero. Samples with
\(g_{2}>0\) are called /leptokurtic/, and samples with \(g_{2}<0\) are
called /platykurtic/. Samples with \(g_{2}\approx0\) are called
/mesokurtic/.

As a rule of thumb, if \(|g_{2}|>4\sqrt{6/n}\) then the sample excess
kurtosis is substantially different from zero in the direction of the
sign of \(g_{2}\). See Tabachnick & Fidell \cite{Tabachnick2006} for
details.

Notice that both the sample skewness and the sample kurtosis are
invariant with respect to location and scale, that is, the values of
\(g_{1}\) and \(g_{2}\) do not depend on the measurement units of the
data.

**** How to do it with \(\mathsf{R}\)

The =e1071= package \cite{e1071} has the =skewness= function for the
sample skewness and the =kurtosis= function for the sample excess
kurtosis. Both functions have a =na.rm= argument which is =FALSE= by
default.

# +BEGIN_exampletoo

We said earlier that the =discoveries= data looked positively skewed;
let's see what the statistics say:
# +END_exampletoo

#+BEGIN_SRC R :exports both :results output pp  
skewness(discoveries)
2*sqrt(6/length(discoveries))
#+END_SRC

The data are definitely skewed to the right. Let us check the sample
excess kurtosis of the =UKDriverDeaths= data:

#+BEGIN_SRC R :exports both :results output pp  
kurtosis(UKDriverDeaths)
4*sqrt(6/length(UKDriverDeaths))
#+END_SRC

so that the =UKDriverDeaths= data appear to be mesokurtic, or at least
not substantially leptokurtic.

** Exploratory Data Analysis
:PROPERTIES:
:CUSTOM_ID: sec-Exploratory-Data-Analysis
:END:

This field was founded (mostly) by John Tukey (1915-2000). Its tools
are useful when not much is known regarding the underlying causes
associated with the data set, and are often used for checking
assumptions. For example, suppose we perform an experiment and collect
some data... now what? We look at the data using exploratory visual
tools.

*** More About Stem-and-leaf Displays

There are many bells and whistles associated with stemplots, and the
=stem.leaf= function can do many of them.

- Trim Outliers: :: Some data sets have observations that fall far
                    from the bulk of the other data (in a sense made
                    more precise in Section [[sub-Outliers][Outliers]]). These extreme
                    observations often obscure the underlying
                    structure to the data and are best left out of the
                    data display. The =trim.outliers= argument (which
                    is =TRUE= by default) will separate the extreme
                    observations from the others and graph the
                    stemplot without them; they are listed at the
                    bottom (respectively, top) of the stemplot with
                    the label =HI= (respectively =LO=).
- Split Stems: :: The standard stemplot has only one line per stem,
                  which means that all observations with first digit
                  =3= are plotted on the same line, regardless of the
                  value of the second digit. But this gives some
                  stemplots a ``skyscraper'' appearance, with too many
                  observations stacked onto the same stem. We can
                  often fix the display by increasing the number of
                  lines available for a given stem. For example, we
                  could make two lines per stem, say, =3*= and
                  =3.=. Observations with second digit 0 through 4
                  would go on the upper line, while observations with
                  second digit 5 through 9 would go on the lower
                  line. (We could do a similar thing with five lines
                  per stem, or even ten lines per stem.) The end
                  result is a more spread out stemplot which often
                  looks better. A good example of this was shown on
                  page \pageref{exa-stemleaf-multiple-lines-stem}.
- Depths: :: these are used to give insight into the balance of the
             observations as they accumulate toward the median. In a
             column beside the standard stemplot, the frequency of the
             stem containing the sample median is shown in
             parentheses. Next, frequencies are accumulated from the
             outside inward, including the outliers. Distributions
             that are more symmetric will have better balanced depths
             on either side of the sample median.

**** How to do it with \(\mathsf{R}\)

The basic command is =stem(x)= or a more sophisticated version written
by Peter Wolf called =stem.leaf(x)= in the \(\mathsf{R}\)
Commander. We will describe =stem.leaf= since that is the one used by
\(\mathsf{R}\) Commander.


WARNING: Sometimes when making a stem-and-leaf display the result will
not be what you expected. There are several reasons for this:
- Stemplots by default will trim extreme observations (defined in
  Section [[sub-Outliers][Outliers]]) from the display. This in some cases will result
  in stemplots that are not as wide as expected.
- The leafs digit is chosen automatically by =stem.leaf= according to
  an algorithm that the computer believes will represent the data
  well. Depending on the choice of the digit, =stem.leaf= may drop
  digits from the data or round the values in unexpected ways.

Let us take a look at the =rivers= data set
# <<ite-stemplot-rivers>>

#+BEGIN_SRC R :exports both :results output pp  
stem.leaf(rivers)
#+END_SRC

The stem-and-leaf display shows a right-skewed shape to the =rivers=
data distribution. Notice that the last digit of each of the data
values were dropped from the display. Notice also that there were
eight extreme observations identified by the computer, and their exact
values are listed at the bottom of the stemplot. Look at the scale on
the left of the stemplot and try to imagine how ridiculous the graph
would have looked had we tried to include enough stems to include
these other eight observations; the stemplot would have stretched over
several pages. Notice finally that we can use the depths to
approximate the sample median for these data. The median lies in the
row identified by =(18)=, which means that the median is the average
of the ninth and tenth observation on that row. Those two values
correspond to =43= and =43=, so a good guess for the median would
be 430. (For the record, the sample median is
\(\widetilde{x}=425\). Recall that stemplots round the data to the
nearest stem-leaf pair.)

Next let us see what the =precip= data look like.

#+BEGIN_SRC R :exports both :results output pp  
stem.leaf(precip)
#+END_SRC

Here is an example of split stems, with two lines per stem. The final
digit of each datum has been dropped for the display. The data appear
to be left skewed with four extreme values to the left and one extreme
value to the right. The sample median is approximately 37 (it turns
out to be 36.6).

*** Hinges and the Five Number Summary
:PROPERTIES:
:CUSTOM_ID: sub-hinges-and-5NS
:END:

Given a data set \(x_{1}\), \(x_{2}\), ..., \(x_{n}\), the hinges are
found by the following method:
- Find the order statistics \(x_{(1)}\), \(x_{(2)}\), ...,
  \(x_{(n)}\).
- The /lower hinge/ \(h_{L}\) is in position \(L=\left\lfloor
  (n+3)/2\right\rfloor / 2\), where the symbol \( \left\lfloor
  x\right\rfloor \) denotes the largest integer less than or equal to
  \(x\). If the position \(L\) is not an integer, then the hinge
  \(h_{L}\) is the average of the adjacent order statistics.
- The /upper hinge/ \(h_{U}\) is in position \(n+1-L\).
Given the hinges, the /five number summary/ (\(5NS\)) is
\begin{equation} 
5NS=(x_{(1)},\ h_{L},\ \tilde{x},\ h_{U},\ x_{(n)}).
\end{equation}
An advantage of the \(5NS\) is that it reduces a potentially large
data set to a shorter list of only five numbers, and further, these
numbers give insight regarding the shape of the data distribution
similar to the sample quantiles in Section [[sub-Order-Statistics-and][Order Statistics]].

**** How to do it with \(\mathsf{R}\)

If the data are stored in a vector =x=, then you can compute the
\(5NS\) with the =fivenum= function.

*** Boxplots
:PROPERTIES:
:CUSTOM_ID: sub-boxplots
:END:

A boxplot is essentially a graphical representation of the \(5NS\). It
can be a handy alternative to a stripchart when the sample size is
large.

A boxplot is constructed by drawing a box alongside the data axis with
sides located at the upper and lower hinges. A line is drawn parallel
to the sides to denote the sample median. Lastly, whiskers are
extended from the sides of the box to the maximum and minimum data
values (more precisely, to the most extreme values that are not
potential outliers, defined below).

Boxplots are good for quick visual summaries of data sets, and the
relative positions of the values in the \(5NS\) are good at indicating
the underlying shape of the data distribution, although perhaps not as
effectively as a histogram. Perhaps the greatest advantage of a
boxplot is that it can help to objectively identify extreme
observations in the data set as described in the next section.

Boxplots are also good because one can visually assess multiple
features of the data set simultaneously:
- Center :: can be estimated by the sample median, \(\tilde{x}\).
- Spread :: can be judged by the width of the box, \(h_{U}-h_{L}\). We
            know that this will be close to the \(IQR\), which can be
            compared to \(s\) and the \(MAD\), perhaps after rescaling
            if appropriate.
- Shape :: is indicated by the relative lengths of the whiskers, and
           the position of the median inside the box. Boxes with
           unbalanced whiskers indicate skewness in the direction of
           the long whisker. Skewed distributions often have the
           median tending in the opposite direction of
           skewness. Kurtosis can be assessed using the box and
           whiskers. A wide box with short whiskers will tend to be
           platykurtic, while a skinny box with wide whiskers
           indicates leptokurtic distributions.
- Extreme observations :: are identified with open circles (see
     below).

*** Outliers
:PROPERTIES:
:CUSTOM_ID: sub-Outliers
:END:

A /potential outlier/ is any observation that falls beyond 1.5 times
the width of the box on either side, that is, any observation less
than \(h_{L}-1.5(h_{U}-h_{L})\) or greater than
\(h_{U}+1.5(h_{U}-h_{L})\). A /suspected outlier/ is any observation
that falls beyond 3 times the width of the box on either side. In
\(\mathsf{R}\), both potential and suspected outliers (if present) are
denoted by open circles; there is no distinction between the two.

When potential outliers are present, the whiskers of the boxplot are
then shortened to extend to the most extreme observation that is not a
potential outlier. If an outlier is displayed in a boxplot, the index
of the observation may be identified in a subsequent plot in =Rcmdr=
by clicking the =Identify outliers with mouse= option in the =Boxplot=
dialog.

What do we do about outliers? They merit further investigation. The
primary goal is to determine why the observation is outlying, if
possible. If the observation is a typographical error, then it should
be corrected before continuing. If the observation is from a subject
that does not belong to the population of interest, then perhaps the
datum should be removed. Otherwise, perhaps the value is hinting at
some hidden structure to the data.

**** How to do it with \(\mathsf{R}\)

The quickest way to visually identify outliers is with a boxplot,
described above. Another way is with the =boxplot.stats= function.

# +BEGIN_exampletoo

The =rivers= data. We will look for potential outliers in the =rivers=
data.

#+BEGIN_SRC R :exports both :results output pp  
boxplot.stats(rivers)$out
#+END_SRC

We may change the =coef= argument to 3 (it is 1.5 by default) to
identify suspected outliers.

#+BEGIN_SRC R :exports both :results output pp  
boxplot.stats(rivers, coef = 3)$out
#+END_SRC

# +END_exampletoo


*** Standardizing variables

It is sometimes useful to compare data sets with each other on a scale
that is independent of the measurement units. Given a set of observed
data \(x_{1}\), \(x_{2}\), ..., \(x_{n}\) we get \(z\) scores, denoted
\(z_{1}\), \(z_{2}\), ..., \(z_{n}\), by means of the following
formula \[ z_{i}=\frac{x_{i}-\overline{x}}{s},\quad
i=1,\,2,\,\ldots,\, n.  \]

**** How to do it with \(\mathsf{R}\)

The =scale= function will rescale a numeric vector (or data frame) by
subtracting the sample mean from each value (column) and/or by
dividing each observation by the sample standard deviation.

** Multivariate Data and Data Frames
:PROPERTIES:
:CUSTOM_ID: sec-multivariate-data
:END:

We have had experience with vectors of data, which are long lists of
numbers. Typically, each entry in the vector is a single measurement
on a subject or experimental unit in the study. We saw in Section
[[sub-Vectors][Vectors]] how to form vectors with the =c= function or the =scan=
function.

However, statistical studies often involve experiments where there are
two (or more) measurements associated with each subject. We display
the measured information in a rectangular array in which each row
corresponds to a subject, and the columns contain the measurements for
each respective variable. For instance, if one were to measure the
height and weight and hair color of each of 11 persons in a research
study, the information could be represented with a rectangular
array. There would be 11 rows. Each row would have the person's height
in the first column and hair color in the second column.

The corresponding objects in \(\mathsf{R}\) are called /data frames/,
and they can be constructed with the =data.frame= function. Each row
is an observation, and each column is a variable.

# +BEGIN_exampletoo

Suppose we have two vectors =x= and =y= and we want to make a data
frame out of them.

#+BEGIN_SRC R :exports code :results silent
x <- 5:8
y <- letters[3:6]
A <- data.frame(v1 = x, v2 = y)
#+END_SRC

# +END_exampletoo


Notice that =x= and =y= are the same length. This is /necessary/. Also
notice that =x= is a numeric vector and =y= is a character vector. We
may choose numeric and character vectors (or even factors) for the
columns of the data frame, but each column must be of exactly one
type. That is, we can have a column for =height= and a column for
=gender=, but we will get an error if we try to mix function =height=
(numeric) and =gender= (character or factor) information in the same
column.

Indexing of data frames is similar to indexing of vectors. To get the
entry in row \(i\) and column \(j\) do =A[i,j]=. We can get entire
rows and columns by omitting the other index.

#+BEGIN_SRC R :exports both :results output pp
A[3, ]
A[ , 1]
A[ , 2]
#+END_SRC

There are several things happening above. Notice that =A[3, ]= gave a
data frame (with the same entries as the third row of =A=) yet =A[ ,
1]= is a numeric vector. =A[ ,2]= is a factor vector because the
default setting for =data.frame= is =stringsAsFactors = TRUE=.

Data frames have a =names= attribute and the names may be extracted
with the =names= function. Once we have the names we may extract given
columns by way of the dollar sign.

#+BEGIN_SRC R :exports both :results output pp
names(A)
A['v1']
#+END_SRC

The above is identical to =A[ ,1]=. 

*** Bivariate Data						       :TODO:
:PROPERTIES:
:CUSTOM_ID: sub-Bivariate-Data
:END:

- Stacked bar charts
- odds ratio and relative risk
- Introduce the sample correlation coefficient.

The *sample Pearson product-moment correlation coefficient*:

\[
r=\frac{\sum_{i=1}^{n}(x_{i}-\overline{x})(y_{i}-\overline{y})}{\sqrt{\sum_{i=1}^{n}(x_{i}-\overline{x})}\sqrt{\sum_{i=1}^{n}(y_{i}-\overline{y})}}
\]
- independent of scale
- \(-1< r <1\)
- measures /strength/ and /direction/ of linear association
- Two-Way Tables. Done with =table=, or in the \(\mathsf{R}\)
  Commander by following =Statistics= \(\triangleright\) =Contingency
  Tables= \(\triangleright\)} =Two-way Tables=. You can also enter and
  analyze a two-way table.
  - table
  - prop.table
  - addmargins
  - rowPercents (Rcmdr)
  - colPercents (Rcmdr)
  - totPercents(Rcmdr)
  - A <- xtabs(~ gender + race, data = RcmdrTestDrive)
  - xtabs( Freq ~ Class + Sex, data = Titanic) from built in table
  - barplot(A, legend.text=TRUE) 
  - barplot(t(A), legend.text=TRUE) 
  - barplot(A, legend.text=TRUE, beside = TRUE)
  - spineplot(gender ~ race, data = RcmdrTestDrive)
  - Spine plot: plots categorical versus categorical
- Scatterplot: look for linear association and correlation. 
  - carb ~ optden, data = Formaldehyde (boring)
  - conc ~ rate, data = Puromycin
  - xyplot(accel ~ dist, data = attenu) nonlinear association
  - xyplot(eruptions ~ waiting, data = faithful) (linear, two groups)
  - xyplot(Petal.Width ~ Petal.Length, data = iris)
  - xyplot(pressure ~ temperature, data = pressure) (exponential growth)
  - xyplot(weight ~ height, data = women) (strong positive linear)

*** Multivariate Data
:PROPERTIES:
:CUSTOM_ID: sub-Multivariate-Data
:END:

Multivariate Data Display

- Multi-Way Tables. You can do this with =table=, or in \(\mathsf{R}\)
  Commander by following =Statistics= \(\triangleright\) =Contingency
  Tables= \(\triangleright\) =Multi-way Tables=.
- Scatterplot matrix. used for displaying pairwise scatterplots
  simultaneously. Again, look for linear association and correlation.
- 3D Scatterplot. See Figure [[fig-3D-scatterplot-trees][3D-scatterplot-trees]]
- =plot(state.region, state.division)= 
- =barplot(table(state.division,state.region), legend.text=TRUE)=

#+BEGIN_SRC :eval never
require(graphics)
mosaicplot(HairEyeColor)
x <- apply(HairEyeColor, c(1, 2), sum)
x
mosaicplot(x, main = "Relation between hair and eye color")
y <- apply(HairEyeColor, c(1, 3), sum)
y
mosaicplot(y, main = "Relation between hair color and sex")
z <- apply(HairEyeColor, c(2, 3), sum)
z
mosaicplot(z, main = "Relation between eye color and sex")
#+END_SRC

** Comparing Populations
:PROPERTIES:
:CUSTOM_ID: sec-Comparing-Data-Sets
:END:

Sometimes we have data from two or more groups (or populations) and we
would like to compare them and draw conclusions. Some issues that we
would like to address:
- Comparing centers and spreads: variation within versus between groups
- Comparing clusters and gaps
- Comparing outliers and unusual features
- Comparing shapes.

*** Numerically

I am thinking here about the =Statistics= \(\triangleright\)
=Numerical Summaries= \(\triangleright\) =Summarize by groups= option
or the =Statistics= \(\triangleright\) =Summaries= \(\triangleright\)
=Table of Statistics= option.

*** Graphically

- Boxplots
  - Variable width: the width of the drawn boxplots are proportional
    to \(\sqrt{n_{i}}\), where \(n_{i}\) is the size of the
    \(i^{\mathrm{th}}\) group. Why? Because many statistics have
    variability proportional to the reciprocal of the square root of
    the sample size.
  - Notches: extend to \(1.58\cdot(h_{U}-h_{L})/\sqrt{n}\). The idea
    is to give roughly a 95% confidence interval for the difference in
    two medians. See Chapter [[cha-Hypothesis-Testing][Hypothesis Testing]].
- Stripcharts
  - stripchart(weight ~ feed, method= "stack", data=chickwts) 
- Bar Graphs
  - barplot(xtabs(Freq ~ Admit + Gender, data = UCBAdmissions))
    stacked bar chart
  - barplot(xtabs(Freq ~ Admit, data = UCBAdmissions))
  - barplot(xtabs(Freq ~ Gender + Admit, data = UCBAdmissions, legend = TRUE, beside = TRUE)  oops, discrimination.
  - barplot(xtabs(Freq ~ Admit+Dept, data = UCBAdmissions), legend = TRUE, beside = TRUE) different departments have different standards
  - barplot(xtabs(Freq ~ Gender+Dept, data = UCBAdmissions), legend = TRUE, beside = TRUE) men mostly applied to easy departments, women mostly applied to difficult departments
  - barplot(xtabs(Freq ~ Gender+Dept, data = UCBAdmissions), legend = TRUE, beside = TRUE)
  - barchart(Admit ~ Freq, data = C)
  - barchart(Admit ~ Freq|Gender, data = C)
  - barchart(Admit ~ Freq | Dept, groups = Gender, data = C)
  - barchart(Admit ~ Freq | Dept, groups = Gender, data = C, auto.key = TRUE)
- Histograms
  - ~ breaks | wool{*}tension, data = warpbreaks
  - ~ weight | feed, data = chickwts
  - ~ weight | group, data = PlantGrowth 
  - ~ count | spray, data = InsectSprays
  - ~ len | dose, data = ToothGrowth
  - ~ decrease | treatment, data = OrchardSprays (or rowpos or colpos)
- Scatterplots

#+BEGIN_SRC R :exports code :eval never
xyplot(Petal.Width ~ Petal.Length, data = iris, group = Species)
#+END_SRC

#+NAME: xyplot-group
#+BEGIN_SRC R :exports none :results graphics silent :file ps/datadesc/xyplot-group.ps
print(xyplot(Petal.Width ~ Petal.Length, data = iris, group = Species))
#+END_SRC

#+NAME: fig-xyplot-group
#+CAPTION[Scatterplot of Petal width versus length in the =iris= data.]: \small Scatterplot of Petal width versus length in the =iris= data.
#+ATTR_LaTeX: :width 0.5\textwidth :placement [ht!]
#+RESULTS: xyplot-group
[[file:ps/datadesc/xyplot-group.ps]]

- Scatterplot matrices
  - splom( ~ cbind(GNP.deflator,GNP,Unemployed,Armed.Forces,Population,Year,Employed),  data = longley) 
  - splom( ~ cbind(pop15,pop75,dpi), data = LifeCycleSavings)
  - splom( ~ cbind(Murder, Assault, Rape), data = USArrests)
  - splom( ~ cbind(CONT, INTG, DMNR), data = USJudgeRatings)
  - splom( ~ cbind(area,peri,shape,perm), data = rock)
  - splom( ~ cbind(Air.Flow, Water.Temp, Acid.Conc., stack.loss), data = stackloss)
  - splom( ~ cbind(Fertility,Agriculture,Examination,Education,Catholic,Infant.Mortality), data = swiss)
  - splom(~ cbind(Fertility,Agriculture,Examination), data = swiss) (positive and negative)

- Dot charts
  - dotchart(USPersonalExpenditure)
  - dotchart(t(USPersonalExpenditure))
  - dotchart(WorldPhones) (transpose is no good)
  - freeny.x is no good, neither is volcano
  - dotchart(UCBAdmissions{[},,1{]})
  - dotplot(Survived ~ Freq | Class, groups = Sex, data = B)
  - dotplot(Admit ~ Freq | Dept, groups = Gender, data = C)

- Mosaic plot
  - mosaic( ~ Survived + Class + Age + Sex, data = Titanic) (or just mosaic(Titanic))
  - mosaic( ~ Admit + Dept + Gender, data = UCBAdmissions)

- Spine plots
  - spineplot(xtabs(Freq ~ Admit + Gender, data = UCBAdmissions))
  - # rescaled barplot
- Quantile-quantile plots: There are two ways to do this. One way is
  to compare two independent samples (of the same
  size). qqplot(x,y). Another way is to compare the sample quantiles
  of one variable to the theoretical quantiles of another
  distribution.

Given two samples \(x_{1}\), \(x_{2}\), ..., \(x_{n}\), and \(y_{1}\),
\(y_{2}\), ..., \(y_{n}\), we may find the order statistics
\(x_{(1)}\leq x_{(2)}\leq\cdots\leq x_{(n)}\) and \(y_{(1)}\leq
y_{(2)}\leq\cdots\leq y_{(n)}\). Next, plot the \(n\) points
\((x_{(1)},y_{(1)})\), \((x_{(2)},y_{(2)})\), ...,
\((x_{(n)},y_{(n)})\).

It is clear that if \(x_{(k)}=y_{(k)}\) for all \(k=1,2,\ldots,n\),
then we will have a straight line. It is also clear that in the real
world, a straight line is NEVER observed, and instead we have a
scatterplot that hopefully had a general linear trend. What do the
rules tell us?
- If the \(y\)-intercept of the line is greater (less) than zero, then
  the center of the \(Y\) data is greater (less) than the center of
  the \(X\) data.
- If the slope of the line is greater (less) than one, then the spread of the \(Y\) data is greater (less) than the spread of the \(X\) data.

*** Lattice Graphics
:PROPERTIES:
:CUSTOM_ID: sub-Lattice-Graphics
:END:

The following types of plots are useful when there is one variable of
interest and there is a factor in the data set by which the variable
is categorized.

It is sometimes nice to set =lattice.options(default.theme = "col.whitebg")=

**** Side by side boxplots

#+BEGIN_SRC R :exports code :eval never
bwplot(~weight | feed, data = chickwts)
#+END_SRC

#+NAME: bwplot
#+BEGIN_SRC R :exports none :results graphics silent :file ps/datadesc/bwplot.ps
print(bwplot(~weight | feed, data = chickwts))
#+END_SRC

#+NAME: fig-bwplot
#+CAPTION: \small Boxplots of =weight= by =feed= type in the =chickwts= data.
#+ATTR_LaTeX: :width 0.5\textwidth :placement [ht!]
#+RESULTS: bwplot
[[file:ps/datadesc/bwplot.ps]]

**** Histograms

#+BEGIN_SRC R :exports code :eval never
histogram(~age | education, data = infert)
#+END_SRC

#+NAME: histograms-lattice
#+BEGIN_SRC R :exports none :results graphics :file ps/datadesc/histograms-lattice.ps
print(histogram(~age | education, data = infert))
#+END_SRC

#+NAME: fig-histograms-lattice
#+CAPTION[Histograms of =age= by =education= level]: \small Histograms of =age= by =education= level from the =infert= data.
#+ATTR_LaTeX: :width 0.5\textwidth :placement [ht!]
#+RESULTS: histograms-lattice
[[file:ps/datadesc/histograms-lattice.ps]]


**** Scatterplots

#+BEGIN_SRC R :exports code :eval never
xyplot(Petal.Length ~ Petal.Width | Species, data = iris)
#+END_SRC

#+NAME: xyplot
#+BEGIN_SRC R :exports none :results graphics :file ps/datadesc/xyplot.ps
print(xyplot(Petal.Length ~ Petal.Width | Species, data = iris))
#+END_SRC

#+NAME: fig-xyplot
#+CAPTION[An =xyplot= of =Petal.Length= versus =Petal.Width= by =Species=]: \small An =xyplot= of =Petal.Length= versus =Petal.Width= by =Species= in the =iris= data.
#+ATTR_LaTeX: :width 0.5\textwidth :placement [ht!]
#+RESULTS: xyplot
[[file:ps/datadesc/xyplot.ps]]

**** Coplots

#+BEGIN_SRC R :exports code :eval never
coplot(conc ~ uptake | Type * Treatment, data = CO2)
#+END_SRC

#+NAME: coplot
#+BEGIN_SRC R :exports none :results graphics :file ps/datadesc/coplot.ps
print(coplot(conc ~ uptake | Type * Treatment, data = CO2))
#+END_SRC

#+NAME: fig-coplot
#+CAPTION: \small A =coplot= of =conc= versus =uptake= by =Type= and =Treatment=.
#+ATTR_LaTeX: :width 0.5\textwidth :placement [ht!]
#+RESULTS: coplot
[[file:ps/datadesc/coplot.ps]]

#+LaTeX: \newpage{}

** Some Remarks about Plotting

Getting your labels to look right

#+BEGIN_SRC R :eval never
library(ggplot2)
a <- qplot(state.division, geom = "bar")
a + opts(axis.text.x = theme_text(angle = -90, hjust = 0))
#+END_SRC

#+BEGIN_SRC R :eval never
hist(precip, freq = FALSE)
lines(density(precip))
qplot(precip, geom = "density")
m <- ggplot(as.data.frame(precip), aes(x = precip))
m + geom_histogram()
m + geom_histogram(aes(y = ..density..)) + geom_density()
#+END_SRC


** Exercises
#+LaTeX: \setcounter{thm}{0}

Open \(\mathsf{R}\) and issue the following commands at the command
line to get started. Note that you need to have the
=RcmdrPlugin.IPSUR= package \cite{RcmdrPlugin.IPSUR} installed, and
for some exercises you need the =e1071= package \cite{e1071}.

#+BEGIN_SRC R :exports code :results silent
library("RcmdrPlugin.IPSUR")
data(RcmdrTestDrive)
attach(RcmdrTestDrive)
names(RcmdrTestDrive)
#+END_SRC

To load the data in the \(\mathsf{R}\) Commander (=Rcmdr=), click the
=Data Set= button, and select =RcmdrTestDrive= as the active data
set. To learn more about the data set and where it comes from, type
=?RcmdrTestDrive= at the command line.

#+begin_xca
# <<xca-summary-RcmdrTestDrive>>

Perform a summary of all variables in =RcmdrTestDrive=. You can do
this with the command =summary(RcmdrTestDrive)=.

Alternatively, you can do this in the =Rcmdr= with the sequence
=Statistics= \(\triangleright\) =Summaries= \(\triangleright\) =Active
Data Set=. Report the values of the summary statistics for each
variable.
#+end_xca

#+begin_xca
Make a table of the =race= variable. Do this with =Statistics=
\(\triangleright\) =Summaries= \(\triangleright\) =Frequency
Distributions - IPSUR...=
1. Which ethnicity has the highest frequency?
1. Which ethnicity has the lowest frequency?
1. Include a bar graph of =race=. Do this with =Graphs=
   \(\triangleright\) =IPSUR - Bar Graph...=
#+end_xca

#+begin_xca
Calculate the average =salary= by the factor =gender=. Do this with
=Statistics= \(\triangleright\) =Summaries= \(\triangleright\) =Table
of Statistics...=
1. Which =gender= has the highest mean =salary=? 
1. Report the highest mean =salary=.
1. Compare the spreads for the genders by calculating the standard
   deviation of =salary= by =gender=. Which =gender= has the biggest
   standard deviation?
1. Make boxplots of =salary= by =gender= with the following method:
   #+begin_quote
   On the =Rcmdr=, click =Graphs= \(\triangleright\) =IPSUR - Boxplot...=
   In the =Variable= box, select =salary=.
   Click the =Plot by groups...= box and select =gender=. Click =OK=.
   Click =OK= to graph the boxplot.
   #+end_quote
   How does the boxplot compare to your answers to (1) and (3)?
#+end_xca

#+begin_xca
For this problem we will study the variable =reduction=.
1. Find the order statistics and store them in a vector =x=. /Hint:/
   =x <- sort(reduction)=
1. Find \(x_{(137)}\), the 137\(^{\mathrm{th}}\) order statistic.
1. Find the IQR.
1. Find the Five Number Summary (5NS).
1. Use the 5NS to calculate what the width of a boxplot of =reduction=
   would be.
1. Compare your answers (3) and (5). Are they the same? If not, are
   they close?
1. Make a boxplot of =reduction=, and include the boxplot in your
   report. You can do this with the =boxplot= function, or in =Rcmdr=
   with =Graphs= \(\triangleright\) =IPSUR - Boxplot...=
1. Are there any potential/suspected outliers? If so, list their
   values. /Hint:/ use your answer to (a).
1. Using the rules discussed in the text, classify answers to (8), if
   any, as /potential/ or /suspected/ outliers.
#+end_xca

#+begin_xca
In this problem we will compare the variables =before= and
=after=. Don't forget =library("e1071")=.
1. Examine the two measures of center for both variables. Judging from
   these measures, which variable has a higher center?
1. Which measure of center is more appropriate for =before=? (You may
   want to look at a boxplot.) Which measure of center is more
   appropriate for =after=?
1. Based on your answer to (2), choose an appropriate measure of
   spread for each variable, calculate it, and report its value. Which
   variable has the biggest spread? (Note that you need to make sure
   that your measures are on the same scale.)
1. Calculate and report the skewness and kurtosis for =before=. Based
   on these values, how would you describe the shape of =before=?
1. Calculate and report the skewness and kurtosis for =after=. Based
   on these values, how would you describe the shape of =after=?
1. Plot histograms of =before= and =after= and compare them to your
   answers to (4) and (5).
#+end_xca

#+begin_xca
Describe the following data sets just as if you were communicating
with an alien, but one who has had a statistics class. Mention the
salient features (data type, important properties, anything
special). Support your answers with the appropriate visual displays
and descriptive statistics.
1. Conversion rates of Euro currencies stored in =euro=.
1. State abbreviations stored in =state.abb=.
#+end_xca


* Time Series                                                    :timeseries:
:PROPERTIES:
:tangle: R/timeseries.R
:CUSTOM_ID: cha-Time-Series
:END:

#+BEGIN_SRC R :exports none :eval never
# Chapter: Time Series
# All code released under GPL Version 3
#+END_SRC

This chapter is still under substantial revision. At any time you can preview any released drafts with the development version of the =IPSUR= package which is available from \(\mathsf{R}\)-Forge:

#+BEGIN_SRC R :exports code :eval never
install.packages("IPSUR", repos="http://R-Forge.R-project.org")
library("IPSUR")
read(IPSUR)
#+END_SRC

#+LaTeX: \appendix


* R Session Information                                            :appendix:
:PROPERTIES:
:CUSTOM_ID: cha-R-Session-Information
:END:

If you ever write the \(\mathsf{R}\) help mailing list with a question, then you should include your session information in the email; it makes the reader's job easier and is requested by the Posting Guide. Here is how to do that, and below is what the output looks like.

#+BEGIN_SRC R :exports both :results output pp 
sessionInfo()
#+END_SRC

#+LaTeX: \vfill{}


#+LaTeX: \vfill{}

#+begin_latex
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{\bibname}
\bibliographystyle{plainurl}
\nocite{*}
\bibliography{IPSUR,Rpackages-2.14.1}
\vfill{}
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{\indexname} 
\printindex{}
#+end_latex

#+BEGIN_SRC R :eval never 
rm(.Random.seed)
try(dir.create("../../data"), silent = TRUE)
save.image(file = "../../data/IPSUR.RData")
tools::resaveRdaFiles('../../data', compress = 'xz')
Stangle(file="IPSUR.Rnw", output="../IPSUR.R", annotate=TRUE)
#+END_SRC





